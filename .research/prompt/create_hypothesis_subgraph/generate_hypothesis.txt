
LLM Name: o3-2025-04-16
Input:
You are a researcher in machine learning. Based on the instructions below, please generate a simple new research method with minimal modifications to existing approaches.

# Instructions:
- Read the research topic described below:
    Improving fine-tuning performance of language models.
- A list of related prior studies is provided. Each entry contains a summary of its title, main contributions, methodologies, results, and limitations:
    {
    "Title": "Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning ",
    "Main Contributions": "AdaLoRA addresses the limitation of existing parameter-efficient fine-tuning methods, such as LoRA, which uniformly distribute parameter budgets across all pre-trained weight matrices, neglecting their varying importance. The key contributions include proposing AdaLoRA, an adaptive budget allocation method that parameterizes incremental updates using Singular Value Decomposition (SVD) to effectively prune singular values of unimportant updates. A novel importance metric is introduced for SVD triplets (singular value and corresponding singular vectors) that considers both parameter sensitivity and uncertainty. Additionally, a global budget scheduler is incorporated to enhance training stability and performance. AdaLoRA demonstrates notable improvements over baselines, especially in low-budget settings, across natural language understanding, question answering, and natural language generation tasks.",
    "Methodology": "AdaLoRA's methodology is built upon SVD-based adaptation and importance-aware rank allocation. Incremental weight updates (∆) are parameterized as ∆ = PΛQ, where P and Q are matrices approximating left/right singular vectors, and Λ is a diagonal matrix of singular values. Orthogonality of P and Q is encouraged through a regularization term, R(P, Q) = ∥P⊤P − I∥2 F + ∥QQ⊤ − I∥2 F, added to the training objective. For adaptive rank allocation, incremental matrices are divided into triplets Gk,i = {Pk,∗i, λk,i, Qk,i∗}. A novel importance metric Sk,i is defined for each triplet, which combines the smoothed sensitivity (I^(t)) and uncertainty (U^(t)) of the singular value and the mean importance of elements in the singular vectors, where s(t)(wij) = I^(t)(wij) ⋅ U^(t)(wij). Singular values corresponding to triplets with low importance scores are pruned (zeroed out). A global budget scheduler manages the total rank b(t) by starting with a higher initial budget and gradually reducing it to a target budget using a cubic schedule, followed by a fixed budget period.",
    "Experimental Setup": "Experiments were conducted using DeBERTaV3-base for Natural Language Understanding (NLU) and Question Answering (QA) tasks, and BART-large for Natural Language Generation (NLG) tasks. NLU evaluation utilized the GLUE benchmark (MNLI, SST-2, CoLA, QQP, QNLI, RTE, MRPC, STS-B). QA performance was assessed on SQuAD v1.1 and SQuADv2.0 datasets. NLG tasks included XSum and CNN/DailyMail. Baselines comprised Full fine-tuning, BitFit, Houlsby adapter, Pfeiffer adapter, and a generalized LoRA applied to all weight matrices (Wq, Wk, Wv, Wf1, Wf2). Performance was measured using standard metrics such as Accuracy, Matthews Correlation Coefficient (Mcc), F1, Exact Match (EM), and ROUGE-1/2/L, across various parameter budget levels (e.g., 0.08% to 2.2% of total pre-trained parameters). The implementation used PyTorch and Huggingface Transformers on NVIDIA V100 GPUs, with tuned hyperparameters including learning rate, batch size, epochs, and regularization coefficients.",
    "Limitations": "One limitation is the modest increase in training time incurred by AdaLoRA (e.g., 11% on MNLI and 16% on SQuADv2) compared to LoRA, attributed to the computation and updating of importance scores for the small incremental matrices. While effective, the importance metric is more complex than simpler magnitude-based pruning methods, involving smoothed sensitivity and uncertainty quantification. The method's core design also relies on a specific SVD parameterization of updates, which, while shown to be advantageous over doublet-wise pruning in LoRA, inherently assumes this structural form for weight updates.",
    "Future Research Directions": "Not mentioned",
    "Experiment Code": "File Path: examples/NLG/src/model.py\nContent:\nclass GPT2Config(object):\n    def __init__(\n        self,\n        vocab_size_or_config_json_file=50257,\n        n_positions=1024,\n        n_ctx=1024,\n        n_embd=768,\n        n_layer=12,\n        n_head=12,\n        layer_norm_epsilon=1e-5,\n        initializer_range=0.02,\n        lora_attn_dim=0,\n        lora_attn_alpha=128,\n        lora_dropout=0.0,\n        lora_r_dropout=0.0,\n        fix_dropout=0.0,\n    ):\n        self.vocab_size = vocab_size_or_config_json_file\n        self.n_ctx = n_ctx\n        self.n_positions = n_positions\n        self.n_embd = n_embd\n        self.n_layer = n_layer\n        self.n_head = n_head\n        self.layer_norm_epsilon = layer_norm_epsilon\n        self.initializer_range = initializer_range\n        self.lora_attn_dim = lora_attn_dim\n        self.lora_attn_alpha = lora_attn_alpha\n        self.lora_dropout = lora_dropout\n        self.lora_r_dropout = lora_r_dropout\n\n        self.fix_dropout = fix_dropout\n\nclass Attention(nn.Module):\n    def __init__(self, nx, n_ctx, config, scale=False):\n        super(Attention, self).__init__()\n        n_state = nx  # in Attention: n_state=768 (nx=n_embd)\n        # [switch nx => n_state from Block to Attention to keep identical to TF implem]\n        \n        assert n_state % config.n_head == 0\n        self.register_buffer(\"bias\", torch.tril(torch.ones(n_ctx, n_ctx)).view(1, 1, n_ctx, n_ctx))\n        self.n_head = config.n_head\n        self.split_size = n_state\n        self.scale = scale\n        self.c_attn = lora.MergedLinear(\n            nx, n_state * 3, \n            r=config.lora_attn_dim, \n            lora_alpha=config.lora_attn_alpha, \n            lora_dropout=config.lora_dropout, \n            enable_lora=[True, False, True], \n            fan_in_fan_out=True,\n            merge_weights=False\n        )\n        self.c_proj = Conv1D(n_state, nx)\n\n        self.config = config\n\nFile Path: examples/NLG/src/gpt2_ft.py\nContent:\nimport loralib as lora\n\nif args.lora_dim > 0:\n    lora.mark_only_lora_as_trainable(lm_net)",
    "Experiment Result": "The repository implements a variant of LoRA (Low-Rank Adaptation) for fine-tuning GPT2 models, which serves as the foundational SVD-based adaptation for AdaLoRA. The incremental weight updates (∆) are implicitly parameterized through `lora.MergedLinear` within the `Attention` module, where `r` (rank) is set by `config.lora_attn_dim`, along with `lora_attn_alpha` and `lora_dropout` parameters. The `loralib` library is used for this implementation.\n\nHowever, the specific adaptive rank allocation mechanisms unique to AdaLoRA, as described in the method, are not present in the provided content. This includes:\n- No explicit code for defining triplets `Gk,i = {Pk,∗i, λk,i, Qk,i∗}`.\n- No implementation of the importance metric `Sk,i` that combines smoothed sensitivity, uncertainty, and mean importance of singular vectors.\n- No mechanism for pruning singular values (`λk,i`) based on these importance scores.\n- No global budget scheduler for dynamically managing the total rank `b(t)`, nor a cubic schedule for reducing it.\n- No explicit regularization term `R(P, Q) = ∥P⊤P − I∥2 F + ∥QQ⊤ − I∥2 F` for encouraging orthogonality is visible in the provided code snippets. \n\nThe `lora_attn_dim` (rank) and `lora_attn_alpha` parameters are static configuration values defined in `GPT2Config`, rather than being dynamically adjusted by a scheduler or importance-aware mechanism."
}{
    "Title": "Parameter-Efficient Fine-Tuning Design Spaces",
    "Main Contributions": "The paper introduces parameter-efficient fine-tuning (PEFT) design spaces to systematically explore tuning structures and strategies, moving beyond hand-crafted methods. It discovers four key design patterns for PEFT: (i) grouping layers in a spindle pattern, (ii) allocating trainable parameters uniformly across layers, (iii) tuning all layer groups, and (iv) assigning proper tuning strategies to different groups. These discovered design patterns lead to new PEFT methods (S4-model and S4-3b-model) that consistently and significantly outperform investigated PEFT strategies across various backbone models (T5, RoBERTa, BART, XLNet) and NLP tasks (GLUE, XSum, WMT 2016 en-ro, SuperGLUE), often exceeding full fine-tuning performance.",
    "Methodology": "The methodology involves defining PEFT design spaces characterized by four components: layer grouping, trainable parameter allocation, tunable groups, and strategy assignment. Starting from a relatively unconstrained design space (S0), the research progressively refines the space (S1 to S4) by comparing the overall quality of models sampled from design spaces enforced with different constraints. A low-compute, low-epoch regime (3 tuning epochs) is used for pattern discovery, evaluating the average performance across GLUE tasks from 100 randomly sampled models. Greedy selection is applied at each stage to identify optimal patterns for each component. Individual PEFT strategies considered for assignment include Adapter, Prefix tuning, BitFit, and LoRA, which can be assigned individually or in combinations.",
    "Experimental Setup": "Pattern discovery was conducted primarily using T5-base and T5-3b backbone models on the GLUE benchmark, measuring average performance (Matthews correlation for CoLA, Spearman for STS-B, accuracy for others). For generalizability evaluation, the discovered patterns were applied to T5-base/3b, RoBERTa-base/large, BART-base/large, and XLNet-base/large across diverse NLP tasks. These tasks included GLUE for classification, XSum for abstractive summarization (ROUGE scores), WMT 2016 en-ro for machine translation (BLEU scores), and SuperGLUE. Trainable parameters were set to 0.5% of the backbone model for most methods (Adapter, Prefix, LoRA, and proposed S4-models) and 0.1% for BitFit. Experiments used Hugging Face implementations, a linear decay scheduler with 0.06 warmup ratio, batch sizes of 128 (base models) or 64 (large models), a maximum learning rate of 5e-5, and 5 or 10 training epochs for final evaluation (3 epochs for discovery). All experiments were performed using 8 A100 GPUs. Baselines included Full Fine-tuning, Adapter, Prefix, BitFit, LoRA, and PA (Parallel Adapter) for generation tasks.",
    "Limitations": "The study acknowledges that, for computational efficiency, it did not enumerate all possible constraints with respect to the design space components, meaning the discovered patterns are optimal within the explored subspace rather than necessarily globally. The progressive refinement approach relies on greedy selection, which might not guarantee finding the globally optimal combination of design patterns. Additionally, the initial pattern discovery process primarily utilized T5 models and classification tasks (GLUE), although the discovered patterns were later shown to generalize well to other models and tasks.",
    "Future Research Directions": "Not mentioned",
    "Experiment Code": "if is_torch_available(): _import_structure[\"adapters\"] = [\"ADAPTER_CACHE\", \"ADAPTER_CONFIG_MAP\", \"ADAPTERFUSION_CONFIG_MAP\", \"ADAPTER_MODEL_MAPPING\", \"DEFAULT_ADAPTER_CONFIG\", \"DEFAULT_ADAPTERFUSION_CONFIG\", \"MODEL_WITH_HEADS_MAPPING\", \"AdapterArguments\", \"AdapterConfig\", \"AdapterConfigBase\", \"AdapterFusionConfig\", \"AdapterInfo\", \"AdapterLayer\", \"AdapterLayerBase\", \"AdapterSetup\", \"AdapterTrainer\", \"AdapterType\", \"AutoAdapterModel\", \"AutoModelWithHeads\", \"BartAdapterModel\", \"BartModelWithHeads\", \"BeitAdapterModel\", \"BertAdapterModel\", \"BertModelWithHeads\", \"CompacterConfig\", \"CompacterPlusPlusConfig\", \"ConfigUnion\", \"DebertaAdapterModel\", \"DebertaV2AdapterModel\", \"DistilBertAdapterModel\", \"DistilBertModelWithHeads\", \"DynamicAdapterFusionConfig\", \"EmbeddingAdaptersMixin\", \"ForwardContext\", \"GPT2AdapterModel\", \"GPT2ModelWithHeads\", \"GPTJAdapterModel\", \"HoulsbyConfig\", \"HoulsbyInvConfig\", \"IA3Config\", \"InvertibleAdaptersMixin\", \"LoRAConfig\", \"MAMConfig\", \"MBartAdapterModel\", \"MBartModelWithHeads\", \"ModelAdaptersConfig\", \"ModelAdaptersMixin\", \"ModelWithFlexibleHeadsAdaptersMixin\", \"ModelWithHeadsAdaptersMixin\", \"MultiLingAdapterArguments\", \"ParallelConfig\", \"PfeifferConfig\", \"PfeifferInvConfig\", \"PrefixTuningConfig\", \"RobertaAdapterModel\", \"RobertaModelWithHeads\", \"Seq2SeqAdapterTrainer\", \"StaticAdapterFusionConfig\", \"T5AdapterModel\", \"T5ModelWithHeads\", \"PEFTConfig\", \"ViTAdapterModel\", \"XLMRobertaAdapterModel\", \"XLMRobertaModelWithHeads\", \"get_adapter_config_hash\", \"get_adapter_info\", \"list_adapters\"]",
    "Experiment Result": "The provided repository content (`setup.py` and `transformers/__init__.py`) indicates that this is \"A friendly fork of HuggingFace's Transformers, adding Adapters to PyTorch language models\" (from `setup.py` description). It lists various adapter-related configurations (e.g., `AdapterConfig`, `LoRAConfig`, `PrefixTuningConfig`) and models, as well as an `AdapterTrainer`. However, specific experimental settings described in the method, such as the \"low-compute, low-epoch regime (3 tuning epochs)\", evaluation across \"GLUE tasks from 100 randomly sampled models\", or the details of the \"greedy selection\" algorithm, are not present in the provided files."
}{
    "Title": "A Gradient Accumulation Method for Dense Retriever under Memory Constraint",
    "Main Contributions": "This research addresses the challenge of training dense retrievers with InfoNCE loss under memory constraints, where large batch sizes are crucial but demand significant hardware resources. Existing memory reduction methods like GradAccum and GradCache suffer from slow and unstable training or fail to surpass high-resource performance. The paper proposes Contrastive Accumulation (CONTACCUM), a novel memory reduction method that achieves stable and efficient training. Its main contributions include: introducing a dual memory bank strategy that allows CONTACCUM to outperform existing memory reduction methods and even high-resource baselines in low-resource settings; demonstrating its time efficiency compared to other memory reduction techniques; and providing mathematical analysis and experimental validation that a dual memory bank strategy stabilizes training by resolving the gradient norm imbalance problem, a major cause of instability in existing memory bank utilization methods.",
    "Methodology": "The proposed method, CONTACCUM, extends the gradient accumulation concept by integrating a dual memory bank structure for both queries (Mq) and passages (Mp). These memory banks are implemented as First-In-First-Out (FIFO) queues that cache previously generated query and passage representations. Unlike standard GradAccum which reduces the number of negative samples per query, CONTACCUM constructs a larger similarity matrix by combining the current local batch representations with the stored representations from both memory banks. A stop-gradient operation is applied to the memory bank representations to prevent back-propagation through them, ensuring computational efficiency. This increases the effective number of negative passages to Nlocal + Np_memory - 1. The paper also provides a detailed gradient analysis of InfoNCE loss with memory banks, revealing that an imbalance in the number of query and passage representations used for gradient calculations (e.g., when only a passage memory bank is used) leads to a 'gradient norm imbalance problem' between the query and passage encoders, causing training instability. CONTACCUM resolves this by employing equally sized dual memory banks (Nq_memory = Np_memory) to ensure balanced gradient norms.",
    "Experimental Setup": "Experiments were conducted on a single A100 80GB GPU, with memory availability strictly controlled to simulate low-resource settings (11GB, 24GB) and a high-resource setting (80GB) using PyTorch's memory fraction function. Five widely used information retrieval datasets were used: Natural Questions (NQ), TriviaQA, Curated TREC (TREC), Web Questions (WebQ) (all preprocessed by DPR, including hard negative samples), and MS Marco (preprocessed from BEIR, with BM25 hard negatives filtered using cross-encoder scores). Evaluation metrics included Top@k for NQ, TriviaQA, TREC, and WebQ, and NDCG@K and Recall@K for MS Marco, evaluated on test or dev sets using the entire document collection. The experimental code was adapted from nano-DPR, utilizing a BERT-base-uncased model. Training epochs varied by dataset (NQ/TREC: 40, TriviaQA/WebQ: 100, MS Marco: 10). Hyperparameters included a warmup step of 1,237, no weight decay, a linear decay learning rate scheduler, AdamW optimizer (epsilon 1e-8, learning rate 2e-5), gradient clipping at 2.0, and τ set to 1. FAISS was used for nearest neighbor search. Optimal memory bank sizes were selected based on evaluation data or dataset size (e.g., NQ: 2,048, MS Marco: 1,024). Baselines included DPR (maximum batch size), GradAccum (Ntotal=128), and GradCache (same Nlocal as CONTACCUM), all trained with hard negatives.",
    "Limitations": "The study's primary limitation is its focus on supervised fine-tuning. It does not investigate whether the gradient norm imbalance problem extends to or can be alleviated by CONTACCUM during the pre-training stage, which often involves uni-encoder structures. Additionally, CONTACCUM still relies on the softmax operation, which inherently incurs high computational costs, limiting potential efficiency gains and broader application where this operation is a bottleneck.",
    "Future Research Directions": "Future work will involve extending CONTACCUM to the pre-training phase, specifically assessing its applicability and effectiveness with uni-encoder structures. Another key direction is to investigate and develop more efficient training strategies that can mitigate the substantial computational burden imposed by the softmax operation. The overall goal is to encourage further research into optimizing dual-encoder training, particularly for low-resource environments within the field of information retrieval.",
    "Experiment Code": "The repository content does not contain the explicit implementation of the “dual memory bank structure for both queries (Mq) and passages (Mp)” as First-In-First-Out (FIFO) queues caching previously generated representations, which is a core component of the CONTACCUM method.\n\nHowever, the training script `train_dpr.py` implements gradient accumulation and a distributed training strategy that increases the effective number of negative samples by pooling current batch embeddings across all GPUs for similarity matrix calculation. This mechanism is similar in its goal to expand the negative sample pool, though distinct from CONTACCUM's memory banks.\n\nRelevant code sections from `train_dpr.py` are:\n\n1.  **Gradient Accumulation Setup**:\n    ```python\n    accelerator = Accelerator(\n        gradient_accumulation_steps=args.gradient_accumulation_steps,\n        log_with='wandb',\n        mixed_precision='no',\n        kwargs_handlers=[kwargs]\n    )\n    ```\n\n2.  **Gradient Accumulation Context**:\n    ```python\n    with accelerator.accumulate(dual_encoder):\n        # ... forward pass ...\n        accelerator.backward(loss)\n    ```\n\n3.  **Distributed Embedding Gathering for Larger Similarity Matrix**:\n    ```python\n    if accelerator.use_distributed:\n        doc_list = [torch.zeros_like(doc_embedding) for _ in range(accelerator.num_processes)]\n        dist.all_gather(tensor_list=doc_list, tensor=doc_embedding.contiguous())\n        doc_list[dist.get_rank()] = doc_embedding\n        doc_embedding = torch.cat(doc_list, dim=0)\n\n        query_list = [torch.zeros_like(query_embedding) for _ in range(accelerator.num_processes)]\n        dist.all_gather(tensor_list=query_list, tensor=query_embedding.contiguous())\n        query_list[dist.get_rank()] = query_embedding\n        query_embedding = torch.cat(query_list, dim=0)\n    ```\n\n4.  **Similarity Score Calculation and InfoNCE Loss**:\n    ```python\n    matching_score = torch.matmul(query_embedding,doc_embedding.permute(1,0))\n    labels = torch.cat([torch.arange(single_device_query_num) + gpu_index * single_device_doc_num for gpu_index in range(accelerator.num_processes)],dim=0).to(matching_score.device)\n    loss = calculate_dpr_loss(matching_score,labels=labels)\n    ```",
    "Experiment Result": "The `train_dpr.py` script describes experimental settings for training a Dual Encoder model, which includes standard gradient accumulation and distributed training for contrastive learning. While the CONTACCUM method's explicit dual memory bank (FIFO queue) for caching *previously generated* representations is not present, the following aspects relate to increasing the effective number of negative samples and training stability:\n\n*   **Gradient Accumulation Steps**: The `gradient_accumulation_steps` parameter is configured via `args.gradient_accumulation_steps`, allowing the accumulation of gradients over multiple mini-batches before an optimizer step. This helps simulate larger effective batch sizes without increasing GPU memory usage for the forward pass.\n*   **Distributed Negative Pool**: In a distributed training environment, the query and document embeddings from the *current* local batch on each GPU are gathered across all processes (`dist.all_gather`). This forms a global pool of embeddings from the current distributed batch, significantly increasing the number of negative samples considered for the InfoNCE loss calculation. The effective number of negative passages per query is thus increased by `accelerator.num_processes - 1` times the local batch size, plus the local batch negatives.\n*   **Loss Calculation**: The `calculate_dpr_loss` function uses `F.nll_loss` on `F.log_softmax`, which is equivalent to InfoNCE loss. The labels are constructed to correctly identify positive pairs across the globally gathered embeddings, ensuring proper alignment for the contrastive loss.\n*   **Model Initialization**: Both query and document encoders are initialized from `BertModel.from_pretrained(args.base_model, add_pooling_layer=False)`, indicating a BERT-based architecture.\n*   **Tokenizer**: `BertTokenizer` is used for tokenizing queries and passages, with a `max_length` of 256.\n*   **Training Data**: Loaded from `args.train_file` (e.g., NQ dataset), with `QADataset.collate_fn` selecting one positive context and one or more hard/regular negative contexts per query.\n*   **Optimization**: AdamW optimizer with a linear learning rate scheduler and gradient clipping (`args.max_grad_norm`).\n*   **Evaluation Frequency**: Validation is performed every `EVAL_STEPS` (configurable by `val_check_interval`), calculating average rank and loss on a development dataset."
}
- Identify the most promising existing method that can be improved with minimal modifications to its objective function or core algorithm.
- Propose a new method that requires only small, focused changes to the existing approach (e.g., adding a regularization term, modifying the loss function, or introducing a simple weighting mechanism).
- Ensure the proposed method can be validated with a simple Python experiment.

# Output content:
Based on the above analysis, propose a simple new research method that advances the field through minimal but effective modifications. Your output should include:

- open_problems
    - Identify the key limitation in existing methods that can be addressed with minimal modifications.
    - Focus on problems that can be solved through simple changes to objective functions or algorithms.

- methods
    - Describe the minimal modification to the existing method (e.g., adding regularization, modifying loss function).
    - Explain the theoretical motivation for this change.
    - Keep the modification simple and focused on the identified problem.

- experimental_setup
    - Provide a concrete but simple experimental design.
    - Specify which datasets and evaluation metrics will be used.
    - Design a straightforward comparison with the base method.

- experimental_code
    - Output the core Python code implementing the proposed modification.
    - Focus only on the key changes to the base method.
    - Keep the code concise and readable.

- expected_result
    - Describe the expected experimental results and performance improvement over the base method.

- expected_conclusion
    - Summarize the practical value of the minimal modification.
    - Explain why this simple change leads to meaningful improvement.
Output:
{
    "open_problems": "AdaLoRA shows that allocating the same LoRA rank to every weight matrix is sub-optimal, but its SVD-based importance metric (sensitivity + uncertainty on every singular triplet) costs an extra 11-16 % training time and requires additional code that is hard to port. Can we obtain most of the accuracy gain of adaptive rank allocation with a much cheaper signal that is already produced during standard back-propagation?",
    "method": "Gradient-Norm Aware Rank Allocation for LoRA (GLoRA)\n1. Keep the ordinary LoRA parameterisation W  ←  W  +  Δ where Δ=AB^T,  rank r.\n2. During the first Twarmup optimisation steps record, for every trainable weight matrix k, the exponential-moving-average (EMA) of its LoRA gradient Frobenius norm:  g_k  ←  β·g_k  +  (1-β)·||∂L/∂Δ_k||_F.\n3. After warm-up compute importance scores  s_k  =  g_k / Σ_j g_j.\n4. Given a global rank budget Rtotal (e.g. 128), assign matrix-specific ranks  r_k  =  round( Rtotal · s_k ).  Ensure Σ_k r_k  =  Rtotal by adding / removing 1 where needed.\n5. Re-initialise each LoRA layer with its new rank (easy because LoRA stores A∈ℝ^{d×r}, B∈ℝ^{d×r}).  The already learned sub-matrices for the kept columns are copied; extra columns are created with the same init as LoRA; dropped columns are discarded.  Training then continues normally.\n\nWhy it should work: the gradient norm is a first-order estimate of how much the loss would change if the associated parameters were perturbed. It therefore correlates with parameter importance but is essentially free to compute (it is produced anyway by back-prop).  Unlike AdaLoRA we do not need an SVD or per-singular-value statistics, so no extra forward / backward passes are introduced.",
    "experimental_setup": "Model: DeBERTa-v3-base (same as AdaLoRA paper).\nTasks: GLUE (MNLI, SST-2, RTE) – three representative tasks to keep the demo light.\nBaselines: (1) Ordinary LoRA with uniform rank r=8 per matrix (≈0.2 % params). (2) AdaLoRA with the same global budget (re-implemented from the official repo). (3) Proposed GLoRA with Rtotal = number of LoRA matrices × 8.\nHyper-parameters: identical to LoRA baseline (lr=2e-4, batch=32, 3 epochs).  Warm-up Twarmup = one epoch, β = 0.9.\nMetrics: task-specific dev set accuracy / Matthew’s Corr.; wall-clock training time.  Each experiment is run with a single V100 for fair timing.",
    "experimental_code": "# -------- glo_ra.py --------\nimport torch, math\nfrom peft import get_peft_model, LoraConfig   # pip install peft\n\nclass GLoRAWrapper:\n    \"\"\"Wrap a HF model with gradient-norm aware rank allocation.\"\"\"\n    def __init__(self, hf_model, r_init=8, total_rank=128, beta=0.9, warmup_steps=1000):\n        self.model = get_peft_model(hf_model, LoraConfig(r=r_init))\n        self.total_rank = total_rank\n        self.beta = beta\n        self.warmup_steps = warmup_steps\n        self.step = 0\n        # collect lora modules\n        self.lora_modules = [m for m in self.model.modules() if hasattr(m, \"lora_A\")]\n        self.g_ema = torch.zeros(len(self.lora_modules), device=hf_model.device)\n        # register hooks to capture grad norms\n        for idx, m in enumerate(self.lora_modules):\n            m.lora_B.register_hook(self._make_hook(idx))\n\n    def _make_hook(self, idx):\n        def hook(grad):\n            gn = grad.norm()  # frobenius\n            self.g_ema[idx] = self.beta * self.g_ema[idx] + (1 - self.beta) * gn\n        return hook\n\n    def step_callback(self):\n        self.step += 1\n        if self.step == self.warmup_steps:\n            # allocate new ranks\n            scores = self.g_ema / self.g_ema.sum()\n            ranks = torch.round(scores * self.total_rank).int().tolist()\n            # exact budget fix\n            diff = self.total_rank - sum(ranks)\n            for i in range(abs(diff)):\n                ranks[i % len(ranks)] += 1 if diff > 0 else -1\n            # rebuild LoRA layers\n            for new_r, mod in zip(ranks, self.lora_modules):\n                old_r = mod.lora_A.weight.size(0)\n                if new_r == old_r:  # nothing to do\n                    continue\n                # keep the first min(old_r, new_r) columns\n                keep = min(old_r, new_r)\n                with torch.no_grad():\n                    A_old, B_old = mod.lora_A.weight.data[:keep], mod.lora_B.weight.data[:keep]\n                    mod.update_layer(r=new_r)  # provided by peft, re-initialises weights\n                    mod.lora_A.weight.data[:keep] = A_old\n                    mod.lora_B.weight.data[:keep] = B_old\n            print(\"[GLoRA] re-allocated ranks\", ranks)\n",
    "expected_result": "1. Accuracy: GLoRA ≈ AdaLoRA > vanilla LoRA.  We expect +0.5-1.0 GLUE score points over uniform LoRA, matching 90-95 % of AdaLoRA’s gain.\n2. Training time: GLoRA ≈ LoRA (within ±2 %), clearly faster than AdaLoRA (which reports +11-16 %).\n3. Parameter count: identical to AdaLoRA, both respect the same global budget.",
    "expected_conclusion": "A single extra bookkeeping line (EMA of gradient norms) is enough to reap most of the benefits of adaptive budget allocation without incurring the computational overhead of AdaLoRA’s SVD-based importance analysis.  This demonstrates that inexpensive first-order signals already available in standard training loops can guide parameter-efficient fine-tuning, making advanced PEFT techniques accessible to practitioners with limited hardware or engineering budget."
}
