
LLM Name: o3-2025-04-16
Input:
You are a cutting-edge AI researcher generating complete, executable code for research paper experiments with Hydra configuration management.

Based on the research method in # Current Research Method and experimental design in # Experimental Design, generate production-ready experiment code that integrates with Hydra for configuration management.

# Instructions: Complete Experiment Code Generation

## Core Requirements
- COMPLETE IMPLEMENTATION: Every component must be fully functional, production-ready, publication-worthy code. No "omitted for brevity", no "simplified version", no TODO, PLACEHOLDER, pass, or ...
- PYTORCH EXCLUSIVELY: Use PyTorch as the deep learning framework
- HYDRA INTEGRATION: Use Hydra to manage all experiment configurations from `config/run/*.yaml` files. Use `config_path="../config"` in all @hydra.main decorators
- COMPLETE DATA PIPELINE: Full data loading and preprocessing implementation. Use `.cache/` as the cache directory for all datasets and models (e.g., for HuggingFace, set `cache_dir=".cache/"`)
- WANDB REQUIRED: WandB is mandatory for metrics logging (except trial_mode validation)

## Hydra Configuration Structure
Each run config file (`config/run/{run_id}.yaml`) contains:
- run_id: Unique identifier for this run
- method: The method name (baseline, proposed, ablation, etc.)
- model: Model-specific parameters (name, architecture details, hyperparameters)
- dataset: Dataset-specific parameters (name, preprocessing settings, split ratios)
- training: Training hyperparameters (learning rate, batch size, epochs, optimizer settings, validation split)
- optuna: Hyperparameter search space definition for Optuna optimization

## Command Line Interface
The generated code must support the following CLI:

**Training (main.py):**
```bash
# Full experiment with WandB logging
uv run python -u -m src.main run={run_id} results_dir={path} mode=full

# Trial mode (validation only, WandB disabled)
uv run python -u -m src.main run={run_id} results_dir={path} mode=trial
```
- `run`: Experiment run_id (matching a run_id from config/run/*.yaml)
- `results_dir`: Output directory (passed from GitHub Actions workflow)
- `mode`: Execution mode (required parameter)
  * `mode=trial`: Lightweight execution for validation (epochs=1, batches limited to 1-2, wandb.mode=disabled, optuna.n_trials=0)
  * `mode=full`: Full experiment execution (wandb.mode=online, full epochs, full Optuna trials)
  * **Code must automatically configure based on mode (e.g., `if cfg.mode == "trial": cfg.wandb.mode = "disabled"; cfg.optuna.n_trials = 0` elif `cfg.mode == "full": cfg.wandb.mode = "online"`)**

**Evaluation (evaluate.py, independent execution):**
```bash
uv run python -m src.evaluate results_dir={path} run_ids='["run-1", "run-2", ...]'
```
- `results_dir`: Directory containing experiment metadata and where outputs will be saved
- `run_ids`: JSON string list of run IDs to evaluate (e.g., '["run-1-proposed-bert-glue", "run-2-baseline-bert-glue"]')
- Executed as a separate workflow after all training runs complete
- **NOT called from main.py**

## Script Structure (ExperimentCode format)
Generate complete code for these files ONLY. Do not create any additional files beyond this structure:

**`src/train.py`**: Single experiment run executor
- Uses Hydra config to load all parameters
- Called as subprocess by main.py
- Responsibilities:
  * Train model with given configuration
  * Initialize WandB: `wandb.init(entity=cfg.wandb.entity, project=cfg.wandb.project, id=cfg.run.run_id, config=OmegaConf.to_container(cfg, resolve=True), resume="allow")`
  * Skip `wandb.init()` if `cfg.wandb.mode == "disabled"` (trial_mode)
  * **Optuna Integration**: If using Optuna for hyperparameter search, DO NOT log intermediate trial results to WandB - only train once with the best hyperparameters after optimization completes and log that final run
  * **Log ALL metrics to WandB comprehensively**:
    - Use `wandb.log()` at each training step/batch/epoch with ALL relevant metrics
    - Log as frequently as possible (per-batch or per-epoch) to capture training dynamics
    - Use CONSISTENT metric names across train.py and evaluate.py (e.g., if train.py logs "train_acc", evaluate.py MUST use run.history(keys=["train_acc",...]))
  * **Save final/best metrics to WandB summary**:
    - Use `wandb.summary["key"] = value` for final results
  * Print WandB run URL to stdout
- **NO results.json, no stdout JSON output, no figure generation**

**`src/evaluate.py`**: Independent evaluation and visualization script
- **Execution**: Run independently via `uv run python -m src.evaluate results_dir={path} run_ids='["run-1", "run-2"]'`
- **NOT called from main.py** - executes as separate workflow after all training completes
- **Responsibilities**:
  * Parse command line arguments:
    - `results_dir`: Output directory path
    - `run_ids`: JSON string list of run IDs (parse with `json.loads(args.run_ids)`)
  * Load WandB config from `config/config.yaml` (in repository root)
  * **Retrieve comprehensive experimental data from WandB API** for specified run_ids:
    ```python
    import json
    api = wandb.Api()
    run_ids = json.loads(args.run_ids)  # Parse JSON string to list
    for run_id in run_ids:
        run = api.run(f"{entity}/{project}/{run_id}")
        history = run.history()  # pandas DataFrame with ALL time-series metrics (train_loss, val_acc, etc.)
        summary = run.summary._json_dict  # Final/best metrics (best_val_acc, final_test_acc, etc.)
        config = dict(run.config)  # Run configuration (hyperparameters, model settings, etc.)
    ```
  * **STEP 1: Per-Run Processing** (for each run_id):
    - Export **comprehensive** run-specific metrics to: `{results_dir}/{run_id}/metrics.json`
    - Generate run-specific figures (learning curves, confusion matrices) to: `{results_dir}/{run_id}/`
    - Each run should have its own subdirectory with its metrics and figures
  * **STEP 2: Aggregated Analysis** (after processing all runs):
    - Export aggregated metrics to: `{results_dir}/comparison/aggregated_metrics.json`
    - Compute secondary/derived metrics (e.g., improvement rate: (proposed - baseline) / baseline)
    - Generate comparison figures to: `{results_dir}/comparison/`:
      * Cross-run comparison charts (bar charts, box plots)
      * Performance metrics tables
      * Statistical significance tests
  * **Figure Generation Guidelines**:
    - Use matplotlib or seaborn with proper legends, annotations, tight_layout
    - For line graphs: annotate significant values (final/best values)
    - For bar graphs: annotate values above each bar
    - Use GLOBALLY UNIQUE image filenames to prevent collisions across different runs and directories**:
      * Per-run figures: `{run_id}_{figure_topic}[_<condition>][_pairN].pdf` (e.g., `run-1-proposed-bert-glue_learning_curve.pdf`)
      * Comparison figures: `comparison_{figure_topic}[_<condition>][_pairN].pdf` (e.g., `comparison_accuracy_bar_chart.pdf`)
  * Print all generated file paths to stdout (both per-run and comparison)

**`src/preprocess.py`**: Complete preprocessing pipeline implementation for the specified datasets

**`src/model.py`**: Complete model architecture implementations for all methods (proposed and comparative methods)

**`src/main.py`**: Main orchestrator
- Receives run_id via Hydra, launches train.py as subprocess, manages logs
- **DOES NOT call evaluate.py** (evaluate.py runs independently in separate workflow)
- Use `@hydra.main(config_path="../config")` since execution is from repository root
- **Mode handling**: Automatically configure based on `cfg.mode`:
  * When `cfg.mode == "trial"`: Set `cfg.wandb.mode = "disabled"`, `cfg.optuna.n_trials = 0`, epochs=1, etc.
  * When `cfg.mode == "full"`: Set `cfg.wandb.mode = "online"` and use full configuration

**`config/config.yaml`**: Main Hydra configuration file
- MUST include WandB configuration:
  ```yaml
  wandb:
    entity: gengaru617-personal
    project: 251106-test
    mode: online  # Automatically set to "disabled" in trial_mode
  ```
- `WANDB_API_KEY` environment variable is automatically available for authentication

**`pyproject.toml`**: Complete project dependencies
- MUST include: `hydra-core`, `wandb` (required)
- Include as needed: `optuna`, `torch`, `transformers`, `datasets`, etc.


## Key Implementation Focus Areas
1. **Hydra-Driven Configuration**: All parameters loaded from run configs dynamically
2. **Algorithm Core**: Full implementation of the proposed method with proper abstraction
3. **Mode-Based Behavior**: Code must automatically configure based on `cfg.mode` ("trial" vs "full")
   - `mode=trial`: Set `cfg.wandb.mode="disabled"`, `cfg.optuna.n_trials=0`, epochs=1, limited batches
   - `mode=full`: Set `cfg.wandb.mode="online"`, use full configuration
4. **Run Execution**: main.py executes a single run_id passed via CLI (GitHub Actions dispatches multiple runs separately)
5. **WandB Integration**: All metrics logged to WandB; train.py does NOT output JSON to stdout or save results.json
6. **Independent Evaluation**: evaluate.py runs separately, fetches data from WandB API, generates all figures



## Code Validation Feedback


**Previous Validation Issue**: Evaluation pipeline is not robust to the format of the configuration it receives and will crash before it can finish the mandatory per-run processing.

Details
1. train.py never writes the hierarchical hydra configuration to results_dir/run_id/config.json.  evaluate.py therefore falls back to the WandB run.config object.
2. The WandB REST API returns a *flattened* dictionary whose keys are strings with dot-separated paths (e.g. "model.backbone"), not the nested structure that build_model() expects (cfg.model.backbone, cfg.method_name, cfg.glo_ra …).  When evaluate.py later converts this flattened dict to an OmegaConf object and calls build_model(), attribute look-ups such as cfg.model.backbone will raise a Validation / AttributeError, so confusion-matrix generation – which the specification requires – will fail and the script terminates.
3. Because of (1)+(2) the experiment cannot be executed end-to-end as required.  All other core requirements appear to be met, but this flaw prevents immediate execution.

Minimal fix
• At the end of training, dump the full hydra cfg to “{results_dir}/{run_id}/config.json” (or YAML) using OmegaConf.to_container.
• In evaluate.py, always load this local file (and only fall back to WandB if it is missing) so that build_model() receives the expected nested structure.
• Alternatively, add a small un-flattening utility in evaluate.py to convert the WandB config into the nested schema expected by build_model().
**Action Required**: Address this issue in the implementation.

**Previous Code (for reference)**:
{"config_yaml": "# config/config.yaml\n# Global Hydra configuration shared across all scripts\n\ndefaults:\n  - _self_\n  - run: null\n\n# ---------------------------------------------------------------------------\n# WandB credentials (overridden to \u0027disabled\u0027 for trial mode) ---------------\nwandb:\n  entity: gengaru617-personal\n  project: 251106-test\n  mode: online\n\n# ---------------------------------------------------------------------------\n# Generic training defaults --------------------------------------------------\ntraining:\n  epochs: 3\n  learning_rate: 2e-4\n  weight_decay: 0.01\n  lr_warmup_steps: 1000\n  gradient_accumulation_steps: 1\n  max_grad_norm: 1.0\n  max_train_batches: null   # limited in trial mode\n  max_val_batches: null     # limited in trial mode\n\n# ---------------------------------------------------------------------------\n# Optuna (disabled unless n_trials \u003e 0) --------------------------------------\noptuna:\n  n_trials: 0\n  direction: maximize\n  search_space: {}\n\n# ---------------------------------------------------------------------------\n# Execution control ----------------------------------------------------------\nmode: full            # full / trial\nresults_dir: ./results\n\n# End of global config -------------------------------------------------------", "evaluate_py": "\"\"\"src/evaluate.py\nComprehensive post-training evaluation \u0026 visualisation script.\n\nThis implementation now fully satisfies the specification by:\n  \u2022 Generating per-run confusion matrices (and heat-map figures)\n  \u2022 Computing secondary / derived metrics (improvement-rates)\n  \u2022 Performing statistical significance tests when applicable\n  \u2022 Writing all artefacts to the mandated directory structure\n\"\"\"\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport math\nimport statistics\nfrom pathlib import Path\nfrom typing import Dict, List, Tuple\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport torch\nimport wandb\nfrom omegaconf import OmegaConf\nfrom scipy import stats\nfrom sklearn.metrics import (classification_report, confusion_matrix,\n                             ConfusionMatrixDisplay)\nfrom torch.utils.data import DataLoader\nfrom transformers import (AutoTokenizer, DataCollatorWithPadding,\n                          AutoModelForSequenceClassification)\n\nfrom src.model import GLoRAWrapper, build_model\n\nCACHE_DIR = \".cache/\"\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# ---------------------------------------------------------------------------\n# Utility helpers -----------------------------------------------------------\n# ---------------------------------------------------------------------------\n\ndef _ensure_dir(path: Path) -\u003e None:\n    path.mkdir(parents=True, exist_ok=True)\n\n\ndef _export_json(obj, path: Path) -\u003e None:\n    with open(path, \"w\") as f:\n        json.dump(obj, f, indent=2)\n\n\ndef _plot_learning_curve(df: pd.DataFrame, metric: str, out_path: Path, title: str) -\u003e None:\n    sns.set(style=\"whitegrid\")\n    plt.figure(figsize=(6, 4))\n    sns.lineplot(x=df[\"_step\"], y=df[metric])\n    plt.title(title)\n    plt.xlabel(\"Training step\")\n    plt.ylabel(metric)\n    plt.tight_layout()\n    plt.savefig(out_path, format=\"pdf\")\n    plt.close()\n\n\ndef _plot_bar(values: Dict[str, float], ylabel: str, out_path: Path, title: str) -\u003e None:\n    sns.set(style=\"whitegrid\")\n    plt.figure(figsize=(max(6, len(values) * 1.3), 4))\n    ax = sns.barplot(x=list(values.keys()), y=list(values.values()))\n    for p in ax.patches:\n        _x = p.get_x() + p.get_width() / 2\n        _y = p.get_height()\n        ax.annotate(f\"{_y:.3f}\", (_x, _y), ha=\"center\", va=\"bottom\")\n    plt.ylabel(ylabel)\n    plt.title(title)\n    plt.tight_layout()\n    plt.savefig(out_path, format=\"pdf\")\n    plt.close()\n\n\ndef _plot_box(data: Dict[str, List[float]], ylabel: str, out_path: Path, title: str) -\u003e None:\n    sns.set(style=\"whitegrid\")\n    plt.figure(figsize=(max(6, len(data) * 1.3), 4))\n    df = pd.DataFrame({k: pd.Series(v) for k, v in data.items()})\n    sns.boxplot(data=df)\n    sns.swarmplot(data=df, color=\"black\", size=3)\n    plt.ylabel(ylabel)\n    plt.title(title)\n    plt.tight_layout()\n    plt.savefig(out_path, format=\"pdf\")\n    plt.close()\n\n\ndef _plot_confusion_matrix(cm: np.ndarray, labels: List[str], out_path: Path, title: str) -\u003e None:\n    sns.set(style=\"whitegrid\")\n    fig, ax = plt.subplots(figsize=(5, 4))\n    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=labels, yticklabels=labels, ax=ax)\n    ax.set_xlabel(\"Predicted\")\n    ax.set_ylabel(\"True\")\n    ax.set_title(title)\n    plt.tight_layout()\n    fig.savefig(out_path, format=\"pdf\")\n    plt.close(fig)\n\n\n# ---------------------------------------------------------------------------\n# Dataset helpers -----------------------------------------------------------\n# ---------------------------------------------------------------------------\n\ndef _get_glue_subset(full_name: str) -\u003e str:\n    assert full_name.startswith(\"glue_\"), \"Dataset name must start with \u0027glue_\u0027\"\n    return full_name.split(\"_\", 1)[1]\n\n\ndef _sentence_keys(task: str) -\u003e Tuple[str, str | None]:\n    mapping: Dict[str, Tuple[str, str | None]] = {\n        \"mnli\": (\"premise\", \"hypothesis\"),\n        \"sst2\": (\"sentence\", None),\n        \"rte\": (\"sentence1\", \"sentence2\"),\n    }\n    if task not in mapping:\n        raise ValueError(f\"Unsupported GLUE task {task}\")\n    return mapping[task]\n\n\ndef _build_val_loader(cfg_dict: Dict) -\u003e Tuple[DataLoader, List[str]]:\n    \"\"\"Build validation DataLoader for the run described by *cfg_dict*.\n\n    Returns (loader, class_labels)\n    \"\"\"\n    from datasets import load_dataset  # local import to avoid cost when not needed\n\n    dataset_cfg = cfg_dict[\"dataset\"]\n    task = _get_glue_subset(dataset_cfg[\"name\"])\n    raw = load_dataset(\"glue\", task, cache_dir=CACHE_DIR)\n    val_split = \"validation_matched\" if task == \"mnli\" else \"validation\"\n\n    class_labels = raw[\"train\"].features[\"label\"].names\n    tokenizer = AutoTokenizer.from_pretrained(cfg_dict[\"model\"][\"backbone\"], cache_dir=CACHE_DIR)\n    max_len = dataset_cfg.get(\"max_length\", 256)\n    bsz = dataset_cfg.get(\"batch_size\", 32)\n\n    k1, k2 = _sentence_keys(task)\n\n    def _tok(batch):\n        if k2 is None:\n            enc = tokenizer(batch[k1], truncation=True, max_length=max_len)\n        else:\n            enc = tokenizer(batch[k1], batch[k2], truncation=True, max_length=max_len)\n        enc[\"labels\"] = batch[\"label\"]\n        return enc\n\n    cols_to_remove = [c for c in raw[val_split].column_names if c != \"label\"]\n    tokenised = raw.map(_tok, batched=True, remove_columns=cols_to_remove)\n\n    collator = DataCollatorWithPadding(tokenizer)\n    loader = DataLoader(tokenised[val_split], batch_size=bsz, shuffle=False, collate_fn=collator)\n    return loader, class_labels\n\n\n# ---------------------------------------------------------------------------\n# Confusion-matrix computation ------------------------------------------------\n# ---------------------------------------------------------------------------\n\ndef _compute_confusion_matrix(run_dir: Path, cfg_dict: Dict) -\u003e Dict:\n    \"\"\"Compute confusion-matrix + classification report for *run* using its best\n    checkpoint.  Returns a dictionary with the artefacts.\n    \"\"\"\n    ckpt_path = run_dir / \"best_model.pt\"\n    if not ckpt_path.exists():\n        print(f\"[Eval] WARNING \u2013 checkpoint {ckpt_path} not found; skipping CM\")\n        return {}\n\n    # -------- data \u0026 model ---------------------------------------------------\n    val_loader, class_names = _build_val_loader(cfg_dict)\n    num_labels = len(class_names)\n\n    # build PEFT-model wrapper then grab underlying model for inference\n    model_wrapper = build_model(OmegaConf.create(cfg_dict), num_labels=num_labels)\n    if isinstance(model_wrapper, GLoRAWrapper):\n        model = model_wrapper.model\n    else:\n        model = model_wrapper\n    model.load_state_dict(torch.load(ckpt_path, map_location=\"cpu\"))\n    model.to(DEVICE).eval()\n\n    all_preds, all_labels = [], []\n    with torch.no_grad():\n        for batch in val_loader:\n            batch = {k: v.to(DEVICE) for k, v in batch.items()}\n            logits = model(**batch).logits\n            preds = torch.argmax(logits, dim=-1).cpu().numpy()\n            labels = batch[\"labels\"].cpu().numpy()\n            all_preds.extend(preds.tolist())\n            all_labels.extend(labels.tolist())\n\n    cm = confusion_matrix(all_labels, all_preds, labels=list(range(num_labels)))\n    cls_report = classification_report(all_labels, all_preds, target_names=class_names, output_dict=True)\n    acc = (np.asarray(all_preds) == np.asarray(all_labels)).mean().item()\n    return {\n        \"confusion_matrix\": cm.tolist(),\n        \"classification_report\": cls_report,\n        \"val_accuracy_from_cm\": acc,\n        \"class_names\": class_names,\n    }\n\n\n# ---------------------------------------------------------------------------\n# MAIN -----------------------------------------------------------------------\n# ---------------------------------------------------------------------------\n\ndef main() -\u003e None:\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"results_dir\", type=str, help=\"Directory to store artefacts \u0026 figures\")\n    parser.add_argument(\"run_ids\", type=str, help=\"JSON encoded list of WandB run IDs\")\n    args = parser.parse_args()\n\n    run_ids: List[str] = json.loads(args.run_ids)\n    results_root = Path(args.results_dir).expanduser().resolve()\n    _ensure_dir(results_root)\n\n    # ---------------- WandB credentials --------------------------------------\n    root_cfg = OmegaConf.load(\"config/config.yaml\")\n    entity, project = root_cfg.wandb.entity, root_cfg.wandb.project\n\n    api = wandb.Api()\n    aggregated_scalars: Dict[str, Dict[str, float]] = {}\n    method_to_vals: Dict[str, List[float]] = {}\n\n    # -------------- per-run processing ---------------------------------------\n    for rid in run_ids:\n        run = api.run(f\"{entity}/{project}/{rid}\")\n        history_df: pd.DataFrame = run.history()\n        summary = dict(run.summary._json_dict)\n        cfg_flat = dict(run.config)\n\n        # We previously stored hierarchical config; try to load if present -----\n        run_dir = results_root / rid\n        _ensure_dir(run_dir)\n        cfg_path = run_dir / \"config.json\"\n        if cfg_path.exists():\n            cfg = json.loads(cfg_path.read_text())\n        else:\n            cfg = cfg_flat  # fall-back\n            _export_json(cfg, cfg_path)\n\n        # ---------------- Confusion-matrix -----------------------------------\n        cm_artifacts = _compute_confusion_matrix(run_dir, cfg)\n        if cm_artifacts:\n            cm = np.array(cm_artifacts[\"confusion_matrix\"])\n            _plot_confusion_matrix(cm, cm_artifacts[\"class_names\"], run_dir / f\"{rid}_confusion_matrix.pdf\", title=\"Validation confusion-matrix\")\n\n        # ---------------- Learning curves ------------------------------------\n        history_df.to_json(run_dir / \"history.json\", orient=\"records\", indent=2)\n        if \"train_loss\" in history_df.columns:\n            _plot_learning_curve(history_df, \"train_loss\", run_dir / f\"{rid}_learning_curve.pdf\", title=\"Training loss\")\n        if \"val_acc\" in history_df.columns:\n            _plot_learning_curve(history_df, \"val_acc\", run_dir / f\"{rid}_val_acc_curve.pdf\", title=\"Validation accuracy\")\n\n        # ---------------- Metrics export -------------------------------------\n        metrics_payload = {\n            \"summary\": summary,\n            **cm_artifacts,\n        }\n        _export_json(metrics_payload, run_dir / \"metrics.json\")\n\n        # Aggregated scalar metrics -------------------------------------------\n        best_val_acc = summary.get(\"best_val_acc\", cm_artifacts.get(\"val_accuracy_from_cm\", float(\"nan\")))\n        time_min = summary.get(\"wall_clock_train_seconds\", float(\"nan\")) / 60.0\n        aggregated_scalars[rid] = {\n            \"best_val_acc\": best_val_acc,\n            \"wall_clock_train_minutes\": time_min,\n            \"method\": cfg.get(\"method\", \"unknown\"),\n        }\n        method_to_vals.setdefault(cfg.get(\"method\", \"unknown\"), []).append(best_val_acc)\n\n        print(f\"[Eval] processed {rid} \u2013 artefacts in {run_dir}\")\n\n    # -------------- aggregated comparison ------------------------------------\n    cmp_dir = results_root / \"comparison\"\n    _ensure_dir(cmp_dir)\n\n    # ----- improvement rates vs baseline run (first run in the list) ---------\n    baseline_id = run_ids[0]\n    baseline_acc = aggregated_scalars[baseline_id][\"best_val_acc\"]\n    improvement_rates = {\n        rid: (vals[\"best_val_acc\"] - baseline_acc) / baseline_acc if not math.isnan(baseline_acc) else float(\"nan\")\n        for rid, vals in aggregated_scalars.items() if rid != baseline_id\n    }\n\n    # ----- statistical significance tests -----------------------------------\n    significance: Dict[str, Dict[str, float]] = {}\n    if len(method_to_vals) \u003e= 2:\n        methods = list(method_to_vals.keys())\n        for i in range(len(methods)):\n            for j in range(i + 1, len(methods)):\n                m1, m2 = methods[i], methods[j]\n                v1, v2 = method_to_vals[m1], method_to_vals[m2]\n                if len(v1) \u003e= 2 and len(v2) \u003e= 2:  # need at least 2 samples each\n                    t_stat, p_val = stats.ttest_ind(v1, v2, equal_var=False)\n                    key = f\"{m1}_vs_{m2}\"\n                    significance[key] = {\"t_statistic\": t_stat, \"p_value\": p_val}\n\n    # ----- write aggregated metrics -----------------------------------------\n    aggregated_out = {\n        \"per_run\": aggregated_scalars,\n        \"improvement_rate_vs_\" + baseline_id: improvement_rates,\n        \"significance_tests\": significance,\n    }\n    _export_json(aggregated_out, cmp_dir / \"aggregated_metrics.json\")\n\n    # ----- visual comparison figures ----------------------------------------\n    acc_vals = {k: v[\"best_val_acc\"] for k, v in aggregated_scalars.items()}\n    _plot_bar(acc_vals, \"Best validation accuracy\", cmp_dir / \"comparison_best_val_acc.pdf\", \"Validation accuracy comparison\")\n\n    time_vals = {k: v[\"wall_clock_train_minutes\"] for k, v in aggregated_scalars.items()}\n    _plot_bar(time_vals, \"Training time (min)\", cmp_dir / \"comparison_training_time.pdf\", \"Training time comparison\")\n\n    if len(method_to_vals) \u003e= 2:\n        _plot_box(method_to_vals, \"Best validation accuracy\", cmp_dir / \"comparison_accuracy_boxplot.pdf\", \"Accuracy distribution per method\")\n\n    # ------------------- stdout artefact list -------------------------------\n    for p in results_root.rglob(\"*.pdf\"):\n        print(p)\n    for p in results_root.rglob(\"*.json\"):\n        print(p)\n\n\nif __name__ == \"__main__\":\n    main()", "main_py": "\"\"\"src/main.py\nMain orchestrator \u2013 applies mode-specific overrides and spawns ``src.train`` as\nsub-process so that each run is executed in its own Hydra context.\n\"\"\"\nfrom __future__ import annotations\n\nimport subprocess\nimport sys\n\nimport hydra\nfrom omegaconf import DictConfig\n\n\n@hydra.main(config_path=\"../config\", config_name=\"config\")\ndef main(cfg: DictConfig) -\u003e None:\n    if \"run\" not in cfg or cfg.run is None:\n        print(\"[Main] You must provide run=\u003crun_id\u003e (file in config/run/)\")\n        sys.exit(1)\n\n    overrides = [\n        f\"run={cfg.run}\",\n        f\"results_dir={cfg.results_dir}\",\n        f\"mode={cfg.mode}\",\n    ]\n\n    if cfg.mode == \"trial\":\n        overrides += [\n            \"wandb.mode=disabled\",\n            \"training.epochs=1\",\n            \"optuna.n_trials=0\",\n            \"training.max_train_batches=2\",\n            \"training.max_val_batches=2\",\n        ]\n    elif cfg.mode == \"full\":\n        overrides += [\"wandb.mode=online\"]\n    else:\n        print(f\"[Main] Unknown mode \u0027{cfg.mode}\u0027 \u2013 expected \u0027trial\u0027 or \u0027full\u0027\")\n        sys.exit(1)\n\n    cmd = [\n        sys.executable,\n        \"-u\",\n        \"-m\",\n        \"src.train\",\n    ] + overrides\n\n    print(\"[Main] Exec:\", \" \".join(cmd))\n    subprocess.run(cmd, check=True)\n\n\nif __name__ == \"__main__\":\n    main()", "model_py": "\"\"\"src/model.py\nModel builder for LoRA, AdaLoRA and the proposed GLoRA.\n\"\"\"\nfrom __future__ import annotations\n\nfrom typing import Any\n\nimport torch\nfrom peft import AdaLoraConfig, LoraConfig, get_peft_model\nfrom transformers import AutoModelForSequenceClassification\n\nCACHE_DIR = \".cache/\"\n\n\nclass GLoRAWrapper:\n    \"\"\"Gradient-norm aware global rank re-allocation wrapper for LoRA.\"\"\"\n\n    def __init__(self, hf_model: torch.nn.Module, r_init: int, total_rank: int, beta: float, warmup_steps: int):\n        self.model = get_peft_model(hf_model, LoraConfig(r=r_init))\n        self.total_rank = int(total_rank)\n        self.beta = beta\n        self.warmup_steps = warmup_steps\n        self.step_idx = 0\n\n        self.lora_modules = [m for m in self.model.modules() if hasattr(m, \"lora_A\") and hasattr(m, \"lora_B\")]\n        self.g_ema = torch.zeros(len(self.lora_modules))\n        for idx, mod in enumerate(self.lora_modules):\n            mod.lora_B.register_hook(self._make_grad_hook(idx))\n\n    # ---------------------------------------------------------------------\n    def _make_grad_hook(self, idx: int):\n        def _hook(grad):\n            gn = grad.norm().detach().cpu()\n            self.g_ema[idx] = self.beta * self.g_ema[idx] + (1 - self.beta) * gn\n            return grad\n        return _hook\n\n    # ---------------------------------------------------------------------\n    def step_callback(self):\n        self.step_idx += 1\n        if self.step_idx == self.warmup_steps:\n            self._reallocate()\n\n    # ---------------------------------------------------------------------\n    def _reallocate(self):\n        scores = self.g_ema / self.g_ema.sum()\n        new_ranks = torch.round(scores * self.total_rank).int().tolist()\n        diff = self.total_rank - sum(new_ranks)\n        for i in range(abs(diff)):\n            new_ranks[i % len(new_ranks)] += 1 if diff \u003e 0 else -1\n\n        for new_r, module in zip(new_ranks, self.lora_modules):\n            old_r = module.lora_A.weight.size(0)\n            if new_r == old_r:\n                continue\n            keep = min(old_r, new_r)\n            with torch.no_grad():\n                A_old = module.lora_A.weight.data[:keep].clone()\n                B_old = module.lora_B.weight.data[:keep].clone()\n                module.update_layer(r=new_r)\n                module.lora_A.weight.data[:keep] = A_old\n                module.lora_B.weight.data[:keep] = B_old\n        print(\"[GLoRA] Re-allocated ranks \u2192\", new_ranks)\n\n\n# -----------------------------------------------------------------------------\n# Public builder --------------------------------------------------------------\n# -----------------------------------------------------------------------------\n\ndef build_model(cfg: Any, num_labels: int):\n    base = AutoModelForSequenceClassification.from_pretrained(cfg.model.backbone, num_labels=num_labels, cache_dir=CACHE_DIR)\n\n    method = str(cfg.method_name).lower()\n    if method == \"glora\":\n        return GLoRAWrapper(\n            base,\n            r_init=cfg.model.peft.init_rank,\n            total_rank=cfg.glo_ra.total_rank_budget,\n            beta=float(cfg.glo_ra.beta_ema),\n            warmup_steps=int(cfg.glo_ra.warmup_steps),\n        )\n    elif method == \"adalora\":\n        ada_cfg = AdaLoraConfig(\n            r=cfg.model.peft.init_rank,\n            target_r=cfg.adalora.total_rank_budget,\n            lora_alpha=16,\n            beta1=cfg.adalora.beta1,\n            beta2=cfg.adalora.beta2,\n            orth_reg_weight=cfg.adalora.orth_reg_weight,\n            tinit=cfg.adalora.adaptation_start_step,\n        )\n        return get_peft_model(base, ada_cfg)\n    else:  # vanilla LoRA baseline\n        lora_cfg = LoraConfig(r=cfg.model.peft.init_rank, lora_alpha=16)\n        return get_peft_model(base, lora_cfg)", "preprocess_py": "\"\"\"src/preprocess.py\nDataset loading \u0026 tokenisation utilities for GLUE tasks.\n\"\"\"\nfrom __future__ import annotations\n\nfrom dataclasses import dataclass\nfrom typing import Dict, Tuple\n\nfrom datasets import load_dataset\nfrom torch.utils.data import DataLoader\nfrom transformers import AutoTokenizer, DataCollatorWithPadding\n\nCACHE_DIR = \".cache/\"\n\n\ndef _get_glue_subset(full_name: str) -\u003e str:\n    assert full_name.startswith(\"glue_\"), \"Dataset name must start with \u0027glue_\u0027\"\n    return full_name.split(\"_\", 1)[1]\n\n\ndef _sentence_keys(task: str) -\u003e Tuple[str, str | None]:\n    mapping: Dict[str, Tuple[str, str | None]] = {\n        \"mnli\": (\"premise\", \"hypothesis\"),\n        \"sst2\": (\"sentence\", None),\n        \"rte\": (\"sentence1\", \"sentence2\"),\n    }\n    if task not in mapping:\n        raise ValueError(f\"Unsupported GLUE task {task}\")\n    return mapping[task]\n\n\n@dataclass\nclass GLUEDataModule:\n    cfg: any  # OmegaConf subtree\n    tokenizer: AutoTokenizer\n\n    train_loader: DataLoader = None  # type: ignore\n    val_loader: DataLoader = None    # type: ignore\n    num_labels: int = 0\n\n    def __post_init__(self):\n        task = _get_glue_subset(self.cfg.dataset.name)\n        raw = load_dataset(\"glue\", task, cache_dir=CACHE_DIR)\n        self.num_labels = raw[\"train\"].features[\"label\"].num_classes\n\n        k1, k2 = _sentence_keys(task)\n\n        def _tok(batch):\n            if k2 is None:\n                enc = self.tokenizer(batch[k1], truncation=True, max_length=self.cfg.dataset.max_length)\n            else:\n                enc = self.tokenizer(batch[k1], batch[k2], truncation=True, max_length=self.cfg.dataset.max_length)\n            enc[\"labels\"] = batch[\"label\"]\n            return enc\n\n        cols_to_remove = [c for c in raw[\"train\"].column_names if c != \"label\"]\n        tokenised = raw.map(_tok, batched=True, remove_columns=cols_to_remove)\n\n        collator = DataCollatorWithPadding(self.tokenizer)\n\n        self.train_loader = DataLoader(\n            tokenised[\"train\"],\n            shuffle=True,\n            batch_size=self.cfg.dataset.batch_size,\n            collate_fn=collator,\n            num_workers=4,\n        )\n\n        val_key = \"validation_matched\" if task == \"mnli\" else \"validation\"\n        self.val_loader = DataLoader(\n            tokenised[val_key],\n            shuffle=False,\n            batch_size=self.cfg.dataset.batch_size,\n            collate_fn=collator,\n            num_workers=4,\n        )", "pyproject_toml": "[project]\nname = \"glora_experiment\"\nversion = \"0.1.0\"\ndependencies = [\n    \"torch\u003e=2.0.0\",\n    \"transformers\u003e=4.37.0\",\n    \"datasets\u003e=2.14.0\",\n    \"peft\u003e=0.5.0\",\n    \"hydra-core\u003e=1.3.2\",\n    \"wandb\u003e=0.16.0\",\n    \"optuna\u003e=3.4.0\",\n    \"pandas\u003e=2.0.0\",\n    \"matplotlib\u003e=3.7.0\",\n    \"seaborn\u003e=0.13.0\",\n    \"scipy\u003e=1.11.0\",\n    \"scikit-learn\u003e=1.3.0\",\n    \"tqdm\u003e=4.66.0\",\n]", "train_py": "\"\"\"src/train.py\nSingle-run training executor with Hydra, Optuna and comprehensive WandB logging.\n\"\"\"\nfrom __future__ import annotations\n\nimport math\nimport time\nfrom pathlib import Path\nfrom typing import Any, Dict, Optional\n\nimport hydra\nimport optuna\nimport torch\nimport wandb\nfrom omegaconf import DictConfig, OmegaConf\nfrom optuna.trial import Trial\nfrom torch.nn.utils import clip_grad_norm_\nfrom torch.optim import AdamW\nfrom transformers import AutoTokenizer, get_linear_schedule_with_warmup\n\nfrom src.model import GLoRAWrapper, build_model\nfrom src.preprocess import GLUEDataModule\n\nCACHE_DIR = \".cache/\"\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# -----------------------------------------------------------------------------\n# Helper utilities ------------------------------------------------------------\n# -----------------------------------------------------------------------------\n\ndef _apply_optuna_params(cfg: DictConfig, params: Dict[str, Any]) -\u003e DictConfig:\n    \"\"\"Return a **deep** copy of *cfg* with *params* injected (dot.notation keys).\"\"\"\n    cfg = OmegaConf.create(OmegaConf.to_container(cfg, resolve=True))\n    for key, value in params.items():\n        node = cfg\n        parts = key.split(\".\")\n        for p in parts[:-1]:\n            if p not in node or node[p] is None:\n                node[p] = {}\n            node = node[p]\n        node[parts[-1]] = value\n    return cfg\n\n\ndef _accuracy(preds: torch.Tensor, labels: torch.Tensor) -\u003e float:\n    return (preds == labels).float().mean().item()\n\n\n# -----------------------------------------------------------------------------\n# Core training routine --------------------------------------------------------\n# -----------------------------------------------------------------------------\n\ndef _train_one_run(cfg: DictConfig, trial: Optional[Trial] = None) -\u003e float:\n    \"\"\"Train **one** configuration.  When *trial* is provided the run is executed\n    as an Optuna trial and WandB logging is **disabled**.\n    \"\"\"\n\n    # ----------------------- Optuna parameter injection -----------------------\n    if trial is not None and cfg.optuna.search_space:\n        sampled: Dict[str, Any] = {}\n        for name, space in cfg.optuna.search_space.items():\n            stype = space.get(\"type\")\n            if stype == \"loguniform\":\n                sampled[name] = trial.suggest_float(name, space.low, space.high, log=True)\n            elif stype == \"uniform\":\n                sampled[name] = trial.suggest_float(name, space.low, space.high)\n            elif stype == \"int\":\n                sampled[name] = trial.suggest_int(name, space.low, space.high)\n            elif stype == \"categorical\":\n                sampled[name] = trial.suggest_categorical(name, space.choices)\n            else:\n                raise ValueError(f\"Unsupported search-space type \u0027{stype}\u0027 for key {name}\")\n        cfg = _apply_optuna_params(cfg, sampled)\n\n    # ----------------------------- WandB --------------------------------------\n    wandb_run: Optional[wandb.wandb_sdk.wandb_run.Run] = None\n    wandb_enabled = trial is None and cfg.wandb.mode != \"disabled\"\n    if wandb_enabled:\n        wandb_run = wandb.init(\n            entity=cfg.wandb.entity,\n            project=cfg.wandb.project,\n            id=cfg.run.run_id,\n            config=OmegaConf.to_container(cfg, resolve=True),\n            resume=\"allow\",\n            mode=cfg.wandb.mode,\n        )\n        print(f\"[WandB] {wandb_run.url}\")\n\n    # ------------------------- Data loading -----------------------------------\n    tokenizer = AutoTokenizer.from_pretrained(cfg.model.backbone, cache_dir=CACHE_DIR)\n    dm = GLUEDataModule(cfg, tokenizer)\n    train_loader, val_loader, num_labels = dm.train_loader, dm.val_loader, dm.num_labels\n\n    # ------------------------- Model construction -----------------------------\n    module_or_wrapper = build_model(cfg, num_labels=num_labels)\n    if isinstance(module_or_wrapper, GLoRAWrapper):\n        model = module_or_wrapper.model\n        glora: Optional[GLoRAWrapper] = module_or_wrapper\n    else:\n        model = module_or_wrapper\n        glora = None\n    model.to(DEVICE)\n\n    # ------------------------- Optimiser \u0026 scheduler --------------------------\n    no_decay = [\"bias\", \"LayerNorm.weight\"]\n    param_groups = [\n        {\"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n         \"weight_decay\": cfg.training.weight_decay},\n        {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n         \"weight_decay\": 0.0},\n    ]\n    optim = AdamW(param_groups, lr=cfg.training.learning_rate)\n\n    updates_per_epoch = math.ceil(len(train_loader) / cfg.training.gradient_accumulation_steps)\n    total_updates = updates_per_epoch * cfg.training.epochs\n    scheduler = get_linear_schedule_with_warmup(\n        optim,\n        num_warmup_steps=cfg.training.lr_warmup_steps,\n        num_training_steps=total_updates,\n    )\n\n    # ------------------------- Training loop ----------------------------------\n    best_val_acc = -1.0\n    global_step = 0\n    start_time = time.time()\n\n    for epoch in range(cfg.training.epochs):\n        model.train()\n        running_loss = 0.0\n        correct, seen = 0, 0\n\n        for step, batch in enumerate(train_loader):\n            if cfg.training.max_train_batches is not None and step \u003e= cfg.training.max_train_batches:\n                break\n\n            batch = {k: v.to(DEVICE) for k, v in batch.items()}\n            out = model(**batch)\n            loss = out.loss / cfg.training.gradient_accumulation_steps\n            loss.backward()\n            running_loss += loss.item()\n\n            preds = torch.argmax(out.logits.detach(), dim=-1)\n            correct += (preds == batch[\"labels\"]).sum().item()\n            seen += preds.size(0)\n\n            if (step + 1) % cfg.training.gradient_accumulation_steps == 0:\n                clip_grad_norm_(model.parameters(), cfg.training.max_grad_norm)\n                optim.step(); scheduler.step(); optim.zero_grad()\n                global_step += 1\n\n                if glora is not None:\n                    glora.step_callback()\n\n                if wandb_run is not None:\n                    wandb.log({\n                        \"train_loss\": loss.item() * cfg.training.gradient_accumulation_steps,\n                        \"train_acc\": correct / seen if seen else 0.0,\n                        \"train_lr\": scheduler.get_last_lr()[0],\n                        \"epoch\": epoch,\n                    }, step=global_step)\n\n        # --------------------- Validation --------------------------------------\n        model.eval()\n        val_correct, val_total, val_loss = 0, 0, 0.0\n        with torch.no_grad():\n            for vstep, batch in enumerate(val_loader):\n                if cfg.training.max_val_batches is not None and vstep \u003e= cfg.training.max_val_batches:\n                    break\n                batch = {k: v.to(DEVICE) for k, v in batch.items()}\n                out = model(**batch)\n                val_loss += out.loss.item()\n                preds = torch.argmax(out.logits, dim=-1)\n                val_correct += (preds == batch[\"labels\"]).sum().item()\n                val_total += preds.size(0)\n        val_acc = val_correct / val_total if val_total else 0.0\n\n        if wandb_run is not None:\n            wandb.log({\n                \"val_loss\": val_loss / max(1, (vstep + 1)),\n                \"val_acc\": val_acc,\n                \"epoch\": epoch,\n            }, step=global_step)\n\n        # --------------------- Checkpoint best ---------------------------------\n        if val_acc \u003e best_val_acc:\n            best_val_acc = val_acc\n            ckpt_dir = Path(cfg.results_dir) / cfg.run.run_id\n            ckpt_dir.mkdir(parents=True, exist_ok=True)\n            torch.save(model.state_dict(), ckpt_dir / \"best_model.pt\")\n\n    wall_clock = time.time() - start_time\n\n    if wandb_run is not None:\n        wandb_run.summary[\"best_val_acc\"] = best_val_acc\n        wandb_run.summary[\"wall_clock_train_seconds\"] = wall_clock\n        wandb_run.finish()\n\n    return best_val_acc\n\n\n# -----------------------------------------------------------------------------\n# Hydra entrypoint -------------------------------------------------------------\n# -----------------------------------------------------------------------------\n\n@hydra.main(config_path=\"../config\", config_name=\"config\")\ndef _hydra_main(cfg: DictConfig) -\u003e None:\n    if cfg.optuna.n_trials and cfg.optuna.n_trials \u003e 0:\n        study = optuna.create_study(direction=cfg.optuna.direction)\n\n        def objective(trial: Trial):\n            cfg_copy = OmegaConf.create(OmegaConf.to_container(cfg, resolve=True))\n            return _train_one_run(cfg_copy, trial)\n\n        study.optimize(objective, n_trials=cfg.optuna.n_trials)\n        print(\"[Optuna] Best parameters:\", study.best_trial.params)\n\n        best_cfg = _apply_optuna_params(cfg, study.best_trial.params)\n        best_val = _train_one_run(best_cfg, trial=None)\n    else:\n        best_val = _train_one_run(cfg, trial=None)\n\n    print(f\"[Train] {cfg.run.run_id} finished \u2013 best val acc = {best_val:.4f}\")\n\n\nif __name__ == \"__main__\":\n    _hydra_main()"}

Fix the issues identified above while preserving the correct parts of the implementation.



# Experimental Environment
NVIDIA A100 or H200
VRAM: 80 GB or more
RAM: 2048 GB or more

# Hypothesis
open_problems='AdaLoRA shows that allocating the same LoRA rank to every weight matrix is sub-optimal, but its SVD-based importance metric (sensitivity + uncertainty on every singular triplet) costs an extra 11-16 % training time and requires additional code that is hard to port. Can we obtain most of the accuracy gain of adaptive rank allocation with a much cheaper signal that is already produced during standard back-propagation?' method='Gradient-Norm Aware Rank Allocation for LoRA (GLoRA)\n1. Keep the ordinary LoRA parameterisation W  ←  W  +  Δ where Δ=AB^T,  rank r.\n2. During the first Twarmup optimisation steps record, for every trainable weight matrix k, the exponential-moving-average (EMA) of its LoRA gradient Frobenius norm:  g_k  ←  β·g_k  +  (1-β)·||∂L/∂Δ_k||_F.\n3. After warm-up compute importance scores  s_k  =  g_k / Σ_j g_j.\n4. Given a global rank budget Rtotal (e.g. 128), assign matrix-specific ranks  r_k  =  round( Rtotal · s_k ).  Ensure Σ_k r_k  =  Rtotal by adding / removing 1 where needed.\n5. Re-initialise each LoRA layer with its new rank (easy because LoRA stores A∈ℝ^{d×r}, B∈ℝ^{d×r}).  The already learned sub-matrices for the kept columns are copied; extra columns are created with the same init as LoRA; dropped columns are discarded.  Training then continues normally.\n\nWhy it should work: the gradient norm is a first-order estimate of how much the loss would change if the associated parameters were perturbed. It therefore correlates with parameter importance but is essentially free to compute (it is produced anyway by back-prop).  Unlike AdaLoRA we do not need an SVD or per-singular-value statistics, so no extra forward / backward passes are introduced.' experimental_setup='Model: DeBERTa-v3-base (same as AdaLoRA paper).\nTasks: GLUE (MNLI, SST-2, RTE) – three representative tasks to keep the demo light.\nBaselines: (1) Ordinary LoRA with uniform rank r=8 per matrix (≈0.2 % params). (2) AdaLoRA with the same global budget (re-implemented from the official repo). (3) Proposed GLoRA with Rtotal = number of LoRA matrices × 8.\nHyper-parameters: identical to LoRA baseline (lr=2e-4, batch=32, 3 epochs).  Warm-up Twarmup = one epoch, β = 0.9.\nMetrics: task-specific dev set accuracy / Matthew’s Corr.; wall-clock training time.  Each experiment is run with a single V100 for fair timing.' experimental_code='# -------- glo_ra.py --------\nimport torch, math\nfrom peft import get_peft_model, LoraConfig   # pip install peft\n\nclass GLoRAWrapper:\n    """Wrap a HF model with gradient-norm aware rank allocation."""\n    def __init__(self, hf_model, r_init=8, total_rank=128, beta=0.9, warmup_steps=1000):\n        self.model = get_peft_model(hf_model, LoraConfig(r=r_init))\n        self.total_rank = total_rank\n        self.beta = beta\n        self.warmup_steps = warmup_steps\n        self.step = 0\n        # collect lora modules\n        self.lora_modules = [m for m in self.model.modules() if hasattr(m, "lora_A")]\n        self.g_ema = torch.zeros(len(self.lora_modules), device=hf_model.device)\n        # register hooks to capture grad norms\n        for idx, m in enumerate(self.lora_modules):\n            m.lora_B.register_hook(self._make_hook(idx))\n\n    def _make_hook(self, idx):\n        def hook(grad):\n            gn = grad.norm()  # frobenius\n            self.g_ema[idx] = self.beta * self.g_ema[idx] + (1 - self.beta) * gn\n        return hook\n\n    def step_callback(self):\n        self.step += 1\n        if self.step == self.warmup_steps:\n            # allocate new ranks\n            scores = self.g_ema / self.g_ema.sum()\n            ranks = torch.round(scores * self.total_rank).int().tolist()\n            # exact budget fix\n            diff = self.total_rank - sum(ranks)\n            for i in range(abs(diff)):\n                ranks[i % len(ranks)] += 1 if diff > 0 else -1\n            # rebuild LoRA layers\n            for new_r, mod in zip(ranks, self.lora_modules):\n                old_r = mod.lora_A.weight.size(0)\n                if new_r == old_r:  # nothing to do\n                    continue\n                # keep the first min(old_r, new_r) columns\n                keep = min(old_r, new_r)\n                with torch.no_grad():\n                    A_old, B_old = mod.lora_A.weight.data[:keep], mod.lora_B.weight.data[:keep]\n                    mod.update_layer(r=new_r)  # provided by peft, re-initialises weights\n                    mod.lora_A.weight.data[:keep] = A_old\n                    mod.lora_B.weight.data[:keep] = B_old\n            print("[GLoRA] re-allocated ranks", ranks)\n' expected_result='1. Accuracy: GLoRA ≈ AdaLoRA > vanilla LoRA.  We expect +0.5-1.0 GLUE score points over uniform LoRA, matching 90-95 % of AdaLoRA’s gain.\n2. Training time: GLoRA ≈ LoRA (within ±2 %), clearly faster than AdaLoRA (which reports +11-16 %).\n3. Parameter count: identical to AdaLoRA, both respect the same global budget.' expected_conclusion='A single extra bookkeeping line (EMA of gradient norms) is enough to reap most of the benefits of adaptive budget allocation without incurring the computational overhead of AdaLoRA’s SVD-based importance analysis.  This demonstrates that inexpensive first-order signals already available in standard training loops can guide parameter-efficient fine-tuning, making advanced PEFT techniques accessible to practitioners with limited hardware or engineering budget.'

# Current Research Method
Gradient-Norm Aware Rank Allocation for LoRA (GLoRA)
1. Keep the ordinary LoRA parameterisation W  ←  W  +  Δ where Δ=AB^T,  rank r.
2. During the first Twarmup optimisation steps record, for every trainable weight matrix k, the exponential-moving-average (EMA) of its LoRA gradient Frobenius norm:  g_k  ←  β·g_k  +  (1-β)·||∂L/∂Δ_k||_F.
3. After warm-up compute importance scores  s_k  =  g_k / Σ_j g_j.
4. Given a global rank budget Rtotal (e.g. 128), assign matrix-specific ranks  r_k  =  round( Rtotal · s_k ).  Ensure Σ_k r_k  =  Rtotal by adding / removing 1 where needed.
5. Re-initialise each LoRA layer with its new rank (easy because LoRA stores A∈ℝ^{d×r}, B∈ℝ^{d×r}).  The already learned sub-matrices for the kept columns are copied; extra columns are created with the same init as LoRA; dropped columns are discarded.  Training then continues normally.

Why it should work: the gradient norm is a first-order estimate of how much the loss would change if the associated parameters were perturbed. It therefore correlates with parameter importance but is essentially free to compute (it is produced anyway by back-prop).  Unlike AdaLoRA we do not need an SVD or per-singular-value statistics, so no extra forward / backward passes are introduced.

# Experimental Design
- Summary: We will verify that the inexpensive gradient-norm aware rank allocation strategy for LoRA (GLoRA) can recover almost all the accuracy benefit of AdaLoRA while keeping training cost identical to vanilla LoRA.  The experiment fine-tunes the encoder-only DeBERTa-v3-base model on the MNLI portion of GLUE using three different parameter-efficient fine-tuning schemes:  (1) ordinary LoRA with a fixed rank of 8 for every projection matrix; (2) AdaLoRA, the published adaptive-rank baseline; (3) the proposed GLoRA.  All runs share the same optimisation schedule and global LoRA-rank budget.  During the first warm-up epoch GLoRA keeps a running EMA of the Frobenius norm of each LoRA gradient; the resulting scores are used to redistribute the fixed total rank across layers.  Because the signal is collected during the normal backward pass, no extra training steps or SVDs are introduced.  At the warm-up boundary the LoRA modules are rebuilt with their newly assigned ranks and training proceeds for the remaining epochs.  We measure development-set accuracy and Matthews correlation on MNLI and record wall-clock training time on one A100 to demonstrate that GLoRA matches AdaLoRA’s accuracy with LoRA-level speed.
- Evaluation metrics: ['Accuracy', 'Matthews Correlation', 'Wall-clock Training Time']

# Experiment Runs

- Run ID: proposed-DeBERTa-v3-base-86M--GLUE-MNLI
  Method: proposed
  Model: DeBERTa-v3-base (86M)
  Dataset: GLUE MNLI
  Config File: config/run/proposed-DeBERTa-v3-base-86M--GLUE-MNLI.yaml
  
  Config Content:
  ```yaml
  run_id: proposed-DeBERTa-v3-base-86M--GLUE-MNLI
method: proposed
method_name: GLoRA
model:
  backbone: microsoft/deberta-v3-base
  peft:
    type: lora
    init_rank: 8
  hidden_size: 768
  num_layers: 12
  num_parameters: 86M
dataset:
  name: glue_mnli
  max_length: 256
  batch_size: 32
training:
  epochs: 3
  optimizer: adamw
  learning_rate: 2e-4
  weight_decay: 0.01
  lr_scheduler: linear
  lr_warmup_steps: 1000
  max_grad_norm: 1.0
  gradient_checkpointing: false
glo_ra:
  total_rank_budget: 128
  beta_ema: 0.9
  warmup_steps: 12000
evaluation:
  metrics: [accuracy, matthews_corrcoef]
  track_training_time: true
hardware:
  gpu_type: a100
  num_gpus: 1
optuna:
  n_trials: 25
  direction: maximize
  metric: validation_accuracy
  search_space:
    learning_rate:
      type: loguniform
      low: 1.0e-4
      high: 5.0e-4
    beta_ema:
      type: uniform
      low: 0.8
      high: 0.99
    total_rank_budget:
      type: categorical
      choices: [96, 128, 160]
    warmup_steps:
      type: int
      low: 500
      high: 1500
    batch_size:
      type: categorical
      choices: [16, 32, 64]

  ```
  

- Run ID: comparative-1-DeBERTa-v3-base-86M--GLUE-MNLI
  Method: comparative-1
  Model: DeBERTa-v3-base (86M)
  Dataset: GLUE MNLI
  Config File: config/run/comparative-1-DeBERTa-v3-base-86M--GLUE-MNLI.yaml
  
  Config Content:
  ```yaml
  run_id: comparative-1-DeBERTa-v3-base-86M--GLUE-MNLI
method: comparative-1
method_name: AdaLoRA
model:
  backbone: microsoft/deberta-v3-base
  peft:
    type: adalora
    init_rank: 8
  hidden_size: 768
  num_layers: 12
  num_parameters: 86M
dataset:
  name: glue_mnli
  max_length: 256
  batch_size: 32
training:
  epochs: 3
  optimizer: adamw
  learning_rate: 2e-4
  weight_decay: 0.01
  lr_scheduler: linear
  lr_warmup_steps: 1000
  max_grad_norm: 1.0
  gradient_checkpointing: false
adalora:
  total_rank_budget: 128
  init_rank: 8
  beta1: 0.9
  beta2: 0.99
  orth_reg_weight: 5.0e-4
  adaptation_start_step: 12000
evaluation:
  metrics: [accuracy, matthews_corrcoef]
  track_training_time: true
hardware:
  gpu_type: a100
  num_gpus: 1
optuna:
  n_trials: 25
  direction: maximize
  metric: validation_accuracy
  search_space:
    learning_rate:
      type: loguniform
      low: 1.0e-4
      high: 5.0e-4
    orth_reg_weight:
      type: loguniform
      low: 1.0e-5
      high: 1.0e-3
    adaptation_start_step:
      type: int
      low: 500
      high: 1500
    total_rank_budget:
      type: categorical
      choices: [96, 128, 160]
    batch_size:
      type: categorical
      choices: [16, 32, 64]

  ```
  


# External Resources (Use these for implementation)

**HuggingFace Models:**

- ID: microsoft/deberta-v3-base



**HuggingFace Datasets:**

- ID: Neuronovo/neuronovo-utc-data-glue-mnli




Generate complete, production-ready experiment code that integrates with Hydra configuration system.
Output:
{
    "train_py": "\"\"\"src/train.py\nSingle-run training executor with Hydra, Optuna and comprehensive WandB logging.\nNow additionally dumps the **full hierarchical Hydra config** to\n``{results_dir}/{run_id}/config.json`` so that evaluation can reliably rebuild\nmodels without relying on the flattened WandB config.\n\"\"\"\nfrom __future__ import annotations\n\nimport json\nimport math\nimport time\nfrom pathlib import Path\nfrom typing import Any, Dict, Optional\n\nimport hydra\nimport optuna\nimport torch\nimport wandb\nfrom omegaconf import DictConfig, OmegaConf\nfrom optuna.trial import Trial\nfrom torch.nn.utils import clip_grad_norm_\nfrom torch.optim import AdamW\nfrom transformers import AutoTokenizer, get_linear_schedule_with_warmup\n\nfrom src.model import GLoRAWrapper, build_model\nfrom src.preprocess import GLUEDataModule\n\nCACHE_DIR = \".cache/\"\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# -----------------------------------------------------------------------------\n# Helper utilities ------------------------------------------------------------\n# -----------------------------------------------------------------------------\n\ndef _apply_optuna_params(cfg: DictConfig, params: Dict[str, Any]) -> DictConfig:\n    \"\"\"Return a **deep** copy of *cfg* with *params* injected (dot.notation keys).\"\"\"\n    cfg = OmegaConf.create(OmegaConf.to_container(cfg, resolve=True))\n    for key, value in params.items():\n        node = cfg\n        parts = key.split(\".\")\n        for p in parts[:-1]:\n            if p not in node or node[p] is None:\n                node[p] = {}\n            node = node[p]\n        node[parts[-1]] = value\n    return cfg\n\n\ndef _accuracy(preds: torch.Tensor, labels: torch.Tensor) -> float:\n    return (preds == labels).float().mean().item()\n\n\ndef _save_cfg(cfg: DictConfig, run_dir: Path) -> None:\n    \"\"\"Dump hierarchical OmegaConf *cfg* to JSON at *run_dir/config.json*.\"\"\"\n    run_dir.mkdir(parents=True, exist_ok=True)\n    cfg_path = run_dir / \"config.json\"\n    with cfg_path.open(\"w\") as f:\n        json.dump(OmegaConf.to_container(cfg, resolve=True), f, indent=2)\n\n\n# -----------------------------------------------------------------------------\n# Core training routine --------------------------------------------------------\n# -----------------------------------------------------------------------------\n\ndef _train_one_run(cfg: DictConfig, trial: Optional[Trial] = None) -> float:\n    \"\"\"Train **one** configuration.  When *trial* is provided the run is executed\n    as an Optuna trial and WandB logging is **disabled**.\n    Returns best validation accuracy.\n    \"\"\"\n\n    # ----------------------- Optuna parameter injection -----------------------\n    if trial is not None and cfg.optuna.search_space:\n        sampled: Dict[str, Any] = {}\n        for name, space in cfg.optuna.search_space.items():\n            stype = space.get(\"type\")\n            if stype == \"loguniform\":\n                sampled[name] = trial.suggest_float(name, space.low, space.high, log=True)\n            elif stype == \"uniform\":\n                sampled[name] = trial.suggest_float(name, space.low, space.high)\n            elif stype == \"int\":\n                sampled[name] = trial.suggest_int(name, space.low, space.high)\n            elif stype == \"categorical\":\n                sampled[name] = trial.suggest_categorical(name, space.choices)\n            else:\n                raise ValueError(f\"Unsupported search-space type '{stype}' for key {name}\")\n        cfg = _apply_optuna_params(cfg, sampled)\n\n    # ----------------------------- WandB --------------------------------------\n    wandb_run: Optional[wandb.wandb_sdk.wandb_run.Run] = None\n    wandb_enabled = trial is None and cfg.wandb.mode != \"disabled\"\n    if wandb_enabled:\n        wandb_run = wandb.init(\n            entity=cfg.wandb.entity,\n            project=cfg.wandb.project,\n            id=cfg.run.run_id,\n            config=OmegaConf.to_container(cfg, resolve=True),\n            resume=\"allow\",\n            mode=cfg.wandb.mode,\n        )\n        print(f\"[WandB] {wandb_run.url}\")\n\n    # ------------------------- Directories & config dump ----------------------\n    run_dir = Path(cfg.results_dir) / cfg.run.run_id\n    _save_cfg(cfg, run_dir)  # save immediately so that even failed runs are recorded\n\n    # ------------------------- Data loading -----------------------------------\n    tokenizer = AutoTokenizer.from_pretrained(cfg.model.backbone, cache_dir=CACHE_DIR)\n    dm = GLUEDataModule(cfg, tokenizer)\n    train_loader, val_loader, num_labels = dm.train_loader, dm.val_loader, dm.num_labels\n\n    # ------------------------- Model construction -----------------------------\n    module_or_wrapper = build_model(cfg, num_labels=num_labels)\n\n    if isinstance(module_or_wrapper, GLoRAWrapper):\n        model = module_or_wrapper.model\n        glora: Optional[GLoRAWrapper] = module_or_wrapper\n    else:\n        model = module_or_wrapper\n        glora = None\n    model.to(DEVICE)\n\n    # ------------------------- Optimiser & scheduler --------------------------\n    no_decay = [\"bias\", \"LayerNorm.weight\"]\n    param_groups = [\n        {\n            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n            \"weight_decay\": cfg.training.weight_decay,\n        },\n        {\n            \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n            \"weight_decay\": 0.0,\n        },\n    ]\n    optim = AdamW(param_groups, lr=cfg.training.learning_rate)\n\n    updates_per_epoch = math.ceil(len(train_loader) / cfg.training.gradient_accumulation_steps)\n    total_updates = updates_per_epoch * cfg.training.epochs\n    scheduler = get_linear_schedule_with_warmup(\n        optim,\n        num_warmup_steps=cfg.training.lr_warmup_steps,\n        num_training_steps=total_updates,\n    )\n\n    # ------------------------- Training loop ----------------------------------\n    best_val_acc = -1.0\n    global_step = 0\n    start_time = time.time()\n\n    for epoch in range(cfg.training.epochs):\n        model.train()\n        running_loss = 0.0\n        correct, seen = 0, 0\n\n        for step, batch in enumerate(train_loader):\n            if cfg.training.max_train_batches is not None and step >= cfg.training.max_train_batches:\n                break\n\n            batch = {k: v.to(DEVICE) for k, v in batch.items()}\n            out = model(**batch)\n            loss = out.loss / cfg.training.gradient_accumulation_steps\n            loss.backward()\n            running_loss += loss.item()\n\n            preds = torch.argmax(out.logits.detach(), dim=-1)\n            correct += (preds == batch[\"labels\"]).sum().item()\n            seen += preds.size(0)\n\n            if (step + 1) % cfg.training.gradient_accumulation_steps == 0:\n                clip_grad_norm_(model.parameters(), cfg.training.max_grad_norm)\n                optim.step(); scheduler.step(); optim.zero_grad()\n                global_step += 1\n\n                if glora is not None:\n                    glora.step_callback()\n\n                if wandb_run is not None:\n                    wandb.log(\n                        {\n                            \"train_loss\": loss.item() * cfg.training.gradient_accumulation_steps,\n                            \"train_acc\": correct / seen if seen else 0.0,\n                            \"train_lr\": scheduler.get_last_lr()[0],\n                            \"epoch\": epoch,\n                        },\n                        step=global_step,\n                    )\n\n        # --------------------- Validation --------------------------------------\n        model.eval()\n        val_correct, val_total, val_loss = 0, 0, 0.0\n        with torch.no_grad():\n            for vstep, batch in enumerate(val_loader):\n                if cfg.training.max_val_batches is not None and vstep >= cfg.training.max_val_batches:\n                    break\n                batch = {k: v.to(DEVICE) for k, v in batch.items()}\n                out = model(**batch)\n                val_loss += out.loss.item()\n                preds = torch.argmax(out.logits, dim=-1)\n                val_correct += (preds == batch[\"labels\"]).sum().item()\n                val_total += preds.size(0)\n        val_acc = val_correct / val_total if val_total else 0.0\n\n        if wandb_run is not None:\n            wandb.log(\n                {\n                    \"val_loss\": val_loss / max(1, (vstep + 1)),\n                    \"val_acc\": val_acc,\n                    \"epoch\": epoch,\n                },\n                step=global_step,\n            )\n\n        # --------------------- Checkpoint best ---------------------------------\n        if val_acc > best_val_acc:\n            best_val_acc = val_acc\n            ckpt_path = run_dir / \"best_model.pt\"\n            torch.save(model.state_dict(), ckpt_path)\n\n    wall_clock = time.time() - start_time\n\n    # ------------- final WandB summary & close -------------------------------\n    if wandb_run is not None:\n        wandb_run.summary[\"best_val_acc\"] = best_val_acc\n        wandb_run.summary[\"wall_clock_train_seconds\"] = wall_clock\n        wandb_run.finish()\n\n    # ----------- ensure final config is still stored (may contain Optuna) ----\n    _save_cfg(cfg, run_dir)\n\n    return best_val_acc\n\n\n# -----------------------------------------------------------------------------\n# Hydra entrypoint -------------------------------------------------------------\n# -----------------------------------------------------------------------------\n\n@hydra.main(config_path=\"../config\", config_name=\"config\")\ndef _hydra_main(cfg: DictConfig) -> None:\n    \"\"\"Hydra entrypoint that orchestrates optional Optuna optimisation followed\n    by (possibly) a final training run with the best hyper-parameters.\"\"\"\n    if cfg.optuna.n_trials and cfg.optuna.n_trials > 0:\n        study = optuna.create_study(direction=cfg.optuna.direction)\n\n        def objective(trial: Trial):\n            cfg_copy = OmegaConf.create(OmegaConf.to_container(cfg, resolve=True))\n            return _train_one_run(cfg_copy, trial)\n\n        study.optimize(objective, n_trials=cfg.optuna.n_trials)\n        print(\"[Optuna] Best parameters:\", study.best_trial.params)\n\n        best_cfg = _apply_optuna_params(cfg, study.best_trial.params)\n        best_val = _train_one_run(best_cfg, trial=None)\n    else:\n        best_val = _train_one_run(cfg, trial=None)\n\n    print(f\"[Train] {cfg.run.run_id} finished – best val acc = {best_val:.4f}\")\n\n\nif __name__ == \"__main__\":\n    _hydra_main()",
    "evaluate_py": "\"\"\"src/evaluate.py\nComprehensive post-training evaluation & visualisation script.\nNow robust to either a hierarchical ``config.json`` dumped by ``train.py`` or the\nflattened configuration returned by the WandB REST API by un-flattening keys of\nshape ``a.b.c`` into the expected nested dictionary.\n\"\"\"\nfrom __future__ import annotations\n\nimport argparse\nimport json\nimport math\nfrom pathlib import Path\nfrom typing import Dict, List, Tuple\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport torch\nimport wandb\nfrom omegaconf import OmegaConf\nfrom scipy import stats\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom torch.utils.data import DataLoader\nfrom transformers import (\n    AutoModelForSequenceClassification,\n    AutoTokenizer,\n    DataCollatorWithPadding,\n)\n\nfrom src.model import GLoRAWrapper, build_model\n\nCACHE_DIR = \".cache/\"\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# ---------------------------------------------------------------------------\n# Utility helpers -----------------------------------------------------------\n# ---------------------------------------------------------------------------\n\ndef _ensure_dir(path: Path) -> None:\n    path.mkdir(parents=True, exist_ok=True)\n\n\ndef _export_json(obj, path: Path) -> None:\n    with open(path, \"w\") as f:\n        json.dump(obj, f, indent=2)\n\n\ndef _plot_learning_curve(df: pd.DataFrame, metric: str, out_path: Path, title: str) -> None:\n    sns.set(style=\"whitegrid\")\n    plt.figure(figsize=(6, 4))\n    sns.lineplot(x=df[\"_step\"], y=df[metric])\n    plt.title(title)\n    plt.xlabel(\"Training step\")\n    plt.ylabel(metric)\n    plt.tight_layout()\n    plt.savefig(out_path, format=\"pdf\")\n    plt.close()\n\n\ndef _plot_bar(values: Dict[str, float], ylabel: str, out_path: Path, title: str) -> None:\n    sns.set(style=\"whitegrid\")\n    plt.figure(figsize=(max(6, len(values) * 1.3), 4))\n    ax = sns.barplot(x=list(values.keys()), y=list(values.values()))\n    for p in ax.patches:\n        _x = p.get_x() + p.get_width() / 2\n        _y = p.get_height()\n        ax.annotate(f\"{_y:.3f}\", (_x, _y), ha=\"center\", va=\"bottom\")\n    plt.ylabel(ylabel)\n    plt.title(title)\n    plt.tight_layout()\n    plt.savefig(out_path, format=\"pdf\")\n    plt.close()\n\n\ndef _plot_box(data: Dict[str, List[float]], ylabel: str, out_path: Path, title: str) -> None:\n    sns.set(style=\"whitegrid\")\n    plt.figure(figsize=(max(6, len(data) * 1.3), 4))\n    df = pd.DataFrame({k: pd.Series(v) for k, v in data.items()})\n    sns.boxplot(data=df)\n    sns.swarmplot(data=df, color=\"black\", size=3)\n    plt.ylabel(ylabel)\n    plt.title(title)\n    plt.tight_layout()\n    plt.savefig(out_path, format=\"pdf\")\n    plt.close()\n\n\ndef _plot_confusion_matrix(cm: np.ndarray, labels: List[str], out_path: Path, title: str) -> None:\n    sns.set(style=\"whitegrid\")\n    fig, ax = plt.subplots(figsize=(5, 4))\n    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=labels, yticklabels=labels, ax=ax)\n    ax.set_xlabel(\"Predicted\")\n    ax.set_ylabel(\"True\")\n    ax.set_title(title)\n    plt.tight_layout()\n    fig.savefig(out_path, format=\"pdf\")\n    plt.close(fig)\n\n\n# ---------------------------------------------------------------------------\n# Dataset helpers -----------------------------------------------------------\n# ---------------------------------------------------------------------------\n\ndef _get_glue_subset(full_name: str) -> str:\n    assert full_name.startswith(\"glue_\"), \"Dataset name must start with 'glue_'\"\n    return full_name.split(\"_\", 1)[1]\n\n\ndef _sentence_keys(task: str) -> Tuple[str, str | None]:\n    mapping: Dict[str, Tuple[str, str | None]] = {\n        \"mnli\": (\"premise\", \"hypothesis\"),\n        \"sst2\": (\"sentence\", None),\n        \"rte\": (\"sentence1\", \"sentence2\"),\n    }\n    if task not in mapping:\n        raise ValueError(f\"Unsupported GLUE task {task}\")\n    return mapping[task]\n\n\ndef _build_val_loader(cfg_dict: Dict) -> Tuple[DataLoader, List[str]]:\n    \"\"\"Return validation DataLoader and list-of-class-names for GLUE task.\"\"\"\n    from datasets import load_dataset  # local import to avoid overhead if unused\n\n    dataset_cfg = cfg_dict[\"dataset\"]\n    task = _get_glue_subset(dataset_cfg[\"name\"])\n    raw = load_dataset(\"glue\", task, cache_dir=CACHE_DIR)\n    val_split = \"validation_matched\" if task == \"mnli\" else \"validation\"\n\n    class_labels = raw[\"train\"].features[\"label\"].names\n    tokenizer = AutoTokenizer.from_pretrained(cfg_dict[\"model\"][\"backbone\"], cache_dir=CACHE_DIR)\n    max_len = dataset_cfg.get(\"max_length\", 256)\n    bsz = dataset_cfg.get(\"batch_size\", 32)\n\n    k1, k2 = _sentence_keys(task)\n\n    def _tok(batch):\n        if k2 is None:\n            enc = tokenizer(batch[k1], truncation=True, max_length=max_len)\n        else:\n            enc = tokenizer(batch[k1], batch[k2], truncation=True, max_length=max_len)\n        enc[\"labels\"] = batch[\"label\"]\n        return enc\n\n    cols_to_remove = [c for c in raw[val_split].column_names if c != \"label\"]\n    tokenised = raw.map(_tok, batched=True, remove_columns=cols_to_remove)\n\n    collator = DataCollatorWithPadding(tokenizer)\n    loader = DataLoader(tokenised[val_split], batch_size=bsz, shuffle=False, collate_fn=collator)\n    return loader, class_labels\n\n\n# ---------------------------------------------------------------------------\n# Configuration helpers ------------------------------------------------------\n# ---------------------------------------------------------------------------\n\ndef _unflatten(flat: Dict[str, Any]) -> Dict[str, Any]:\n    \"\"\"Convert a flattened dict with dot-separated keys into a nested dict.\"\"\"\n    nested: Dict[str, Any] = {}\n    for key, val in flat.items():\n        node = nested\n        parts = key.split(\".\")\n        for p in parts[:-1]:\n            node = node.setdefault(p, {})\n        node[parts[-1]] = val\n    return nested\n\n\n# ---------------------------------------------------------------------------\n# Confusion matrix computation ----------------------------------------------\n# ---------------------------------------------------------------------------\n\ndef _compute_confusion_matrix(run_dir: Path, cfg_dict: Dict) -> Dict:\n    ckpt_path = run_dir / \"best_model.pt\"\n    if not ckpt_path.exists():\n        print(f\"[Eval] WARNING – checkpoint {ckpt_path} not found; skipping CM\")\n        return {}\n\n    # data & model -----------------------------------------------\n    val_loader, class_names = _build_val_loader(cfg_dict)\n    num_labels = len(class_names)\n\n    model_wrapper = build_model(OmegaConf.create(cfg_dict), num_labels=num_labels)\n    model = model_wrapper.model if isinstance(model_wrapper, GLoRAWrapper) else model_wrapper\n    model.load_state_dict(torch.load(ckpt_path, map_location=\"cpu\"))\n    model.to(DEVICE).eval()\n\n    all_preds, all_labels = [], []\n    with torch.no_grad():\n        for batch in val_loader:\n            batch = {k: v.to(DEVICE) for k, v in batch.items()}\n            logits = model(**batch).logits\n            preds = torch.argmax(logits, dim=-1).cpu().numpy()\n            labels = batch[\"labels\"].cpu().numpy()\n            all_preds.extend(preds.tolist())\n            all_labels.extend(labels.tolist())\n\n    cm = confusion_matrix(all_labels, all_preds, labels=list(range(num_labels)))\n    cls_report = classification_report(all_labels, all_preds, target_names=class_names, output_dict=True)\n    acc = (np.asarray(all_preds) == np.asarray(all_labels)).mean().item()\n    return {\n        \"confusion_matrix\": cm.tolist(),\n        \"classification_report\": cls_report,\n        \"val_accuracy_from_cm\": acc,\n        \"class_names\": class_names,\n    }\n\n\n# ---------------------------------------------------------------------------\n# MAIN -----------------------------------------------------------------------\n# ---------------------------------------------------------------------------\n\ndef main() -> None:\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"results_dir\", type=str, help=\"Directory to store artefacts & figures\")\n    parser.add_argument(\"run_ids\", type=str, help=\"JSON encoded list of WandB run IDs\")\n    args = parser.parse_args()\n\n    run_ids: List[str] = json.loads(args.run_ids)\n    results_root = Path(args.results_dir).expanduser().resolve()\n    _ensure_dir(results_root)\n\n    # ---------------- WandB credentials --------------------------------------\n    root_cfg = OmegaConf.load(\"config/config.yaml\")\n    entity, project = root_cfg.wandb.entity, root_cfg.wandb.project\n\n    api = wandb.Api()\n    aggregated_scalars: Dict[str, Dict[str, float]] = {}\n    method_to_vals: Dict[str, List[float]] = {}\n\n    # -------------- per-run processing ---------------------------------------\n    for rid in run_ids:\n        run = api.run(f\"{entity}/{project}/{rid}\")\n        history_df: pd.DataFrame = run.history()\n        summary = dict(run.summary._json_dict)\n        cfg_flat = dict(run.config)\n\n        # Prefer local hierarchical config produced during training -------------\n        run_dir = results_root / rid\n        _ensure_dir(run_dir)\n        cfg_path = run_dir / \"config.json\"\n        if cfg_path.exists():\n            cfg = json.loads(cfg_path.read_text())\n        else:\n            # fall-back – un-flatten WandB config\n            cfg = _unflatten(cfg_flat)\n            _export_json(cfg, cfg_path)\n\n        # ---------------- Confusion-matrix ------------------------------------\n        cm_artifacts = _compute_confusion_matrix(run_dir, cfg)\n        if cm_artifacts:\n            cm = np.array(cm_artifacts[\"confusion_matrix\"])\n            _plot_confusion_matrix(\n                cm,\n                cm_artifacts[\"class_names\"],\n                run_dir / f\"{rid}_confusion_matrix.pdf\",\n                title=\"Validation confusion-matrix\",\n            )\n\n        # ---------------- Learning curves ------------------------------------\n        history_df.to_json(run_dir / \"history.json\", orient=\"records\", indent=2)\n        if \"train_loss\" in history_df.columns:\n            _plot_learning_curve(history_df, \"train_loss\", run_dir / f\"{rid}_learning_curve.pdf\", title=\"Training loss\")\n        if \"val_acc\" in history_df.columns:\n            _plot_learning_curve(history_df, \"val_acc\", run_dir / f\"{rid}_val_acc_curve.pdf\", title=\"Validation accuracy\")\n\n        # ---------------- Metrics export -------------------------------------\n        metrics_payload = {\n            \"summary\": summary,\n            **cm_artifacts,\n        }\n        _export_json(metrics_payload, run_dir / \"metrics.json\")\n\n        # Aggregated scalar metrics -------------------------------------------\n        best_val_acc = summary.get(\"best_val_acc\", cm_artifacts.get(\"val_accuracy_from_cm\", float(\"nan\")))\n        time_min = summary.get(\"wall_clock_train_seconds\", float(\"nan\")) / 60.0\n        aggregated_scalars[rid] = {\n            \"best_val_acc\": best_val_acc,\n            \"wall_clock_train_minutes\": time_min,\n            \"method\": cfg.get(\"method\", \"unknown\"),\n        }\n        method_to_vals.setdefault(cfg.get(\"method\", \"unknown\"), []).append(best_val_acc)\n\n        print(f\"[Eval] processed {rid} – artefacts in {run_dir}\")\n\n    # -------------- aggregated comparison ------------------------------------\n    cmp_dir = results_root / \"comparison\"\n    _ensure_dir(cmp_dir)\n\n    # ----- improvement rates vs baseline run (first run in the list) ---------\n    baseline_id = run_ids[0]\n    baseline_acc = aggregated_scalars[baseline_id][\"best_val_acc\"]\n    improvement_rates = {\n        rid: (vals[\"best_val_acc\"] - baseline_acc) / baseline_acc if not math.isnan(baseline_acc) else float(\"nan\")\n        for rid, vals in aggregated_scalars.items() if rid != baseline_id\n    }\n\n    # ----- statistical significance tests -----------------------------------\n    significance: Dict[str, Dict[str, float]] = {}\n    if len(method_to_vals) >= 2:\n        methods = list(method_to_vals.keys())\n        for i in range(len(methods)):\n            for j in range(i + 1, len(methods)):\n                m1, m2 = methods[i], methods[j]\n                v1, v2 = method_to_vals[m1], method_to_vals[m2]\n                if len(v1) >= 2 and len(v2) >= 2:  # need at least 2 samples each\n                    t_stat, p_val = stats.ttest_ind(v1, v2, equal_var=False)\n                    key = f\"{m1}_vs_{m2}\"\n                    significance[key] = {\"t_statistic\": t_stat, \"p_value\": p_val}\n\n    # ----- write aggregated metrics -----------------------------------------\n    aggregated_out = {\n        \"per_run\": aggregated_scalars,\n        \"improvement_rate_vs_\" + baseline_id: improvement_rates,\n        \"significance_tests\": significance,\n    }\n    _export_json(aggregated_out, cmp_dir / \"aggregated_metrics.json\")\n\n    # ----- visual comparison figures ----------------------------------------\n    acc_vals = {k: v[\"best_val_acc\"] for k, v in aggregated_scalars.items()}\n    _plot_bar(acc_vals, \"Best validation accuracy\", cmp_dir / \"comparison_best_val_acc.pdf\", \"Validation accuracy comparison\")\n\n    time_vals = {k: v[\"wall_clock_train_minutes\"] for k, v in aggregated_scalars.items()}\n    _plot_bar(time_vals, \"Training time (min)\", cmp_dir / \"comparison_training_time.pdf\", \"Training time comparison\")\n\n    if len(method_to_vals) >= 2:\n        _plot_box(method_to_vals, \"Best validation accuracy\", cmp_dir / \"comparison_accuracy_boxplot.pdf\", \"Accuracy distribution per method\")\n\n    # ------------------- stdout artefact list -------------------------------\n    for p in results_root.rglob(\"*.pdf\"):\n        print(p)\n    for p in results_root.rglob(\"*.json\"):\n        print(p)\n\n\nif __name__ == \"__main__\":\n    main()",
    "preprocess_py": "\"\"\"src/preprocess.py\nDataset loading & tokenisation utilities for GLUE tasks.\n\"\"\"\nfrom __future__ import annotations\n\nfrom dataclasses import dataclass\nfrom typing import Dict, Tuple\n\nfrom datasets import load_dataset\nfrom torch.utils.data import DataLoader\nfrom transformers import AutoTokenizer, DataCollatorWithPadding\n\nCACHE_DIR = \".cache/\"\n\n\ndef _get_glue_subset(full_name: str) -> str:\n    assert full_name.startswith(\"glue_\"), \"Dataset name must start with 'glue_'\"\n    return full_name.split(\"_\", 1)[1]\n\n\ndef _sentence_keys(task: str) -> Tuple[str, str | None]:\n    mapping: Dict[str, Tuple[str, str | None]] = {\n        \"mnli\": (\"premise\", \"hypothesis\"),\n        \"sst2\": (\"sentence\", None),\n        \"rte\": (\"sentence1\", \"sentence2\"),\n    }\n    if task not in mapping:\n        raise ValueError(f\"Unsupported GLUE task {task}\")\n    return mapping[task]\n\n\n@dataclass\nclass GLUEDataModule:\n    cfg: any  # OmegaConf subtree\n    tokenizer: AutoTokenizer\n\n    train_loader: DataLoader = None  # type: ignore\n    val_loader: DataLoader = None    # type: ignore\n    num_labels: int = 0\n\n    def __post_init__(self):\n        task = _get_glue_subset(self.cfg.dataset.name)\n        raw = load_dataset(\"glue\", task, cache_dir=CACHE_DIR)\n        self.num_labels = raw[\"train\"].features[\"label\"].num_classes\n\n        k1, k2 = _sentence_keys(task)\n\n        def _tok(batch):\n            if k2 is None:\n                enc = self.tokenizer(batch[k1], truncation=True, max_length=self.cfg.dataset.max_length)\n            else:\n                enc = self.tokenizer(batch[k1], batch[k2], truncation=True, max_length=self.cfg.dataset.max_length)\n            enc[\"labels\"] = batch[\"label\"]\n            return enc\n\n        cols_to_remove = [c for c in raw[\"train\"].column_names if c != \"label\"]\n        tokenised = raw.map(_tok, batched=True, remove_columns=cols_to_remove)\n\n        collator = DataCollatorWithPadding(self.tokenizer)\n\n        self.train_loader = DataLoader(\n            tokenised[\"train\"],\n            shuffle=True,\n            batch_size=self.cfg.dataset.batch_size,\n            collate_fn=collator,\n            num_workers=4,\n        )\n\n        val_key = \"validation_matched\" if task == \"mnli\" else \"validation\"\n        self.val_loader = DataLoader(\n            tokenised[val_key],\n            shuffle=False,\n            batch_size=self.cfg.dataset.batch_size,\n            collate_fn=collator,\n            num_workers=4,\n        )",
    "model_py": "\"\"\"src/model.py\nModel builder for LoRA, AdaLoRA and the proposed GLoRA.\n\"\"\"\nfrom __future__ import annotations\n\nfrom typing import Any\n\nimport torch\nfrom peft import AdaLoraConfig, LoraConfig, get_peft_model\nfrom transformers import AutoModelForSequenceClassification\n\nCACHE_DIR = \".cache/\"\n\n\nclass GLoRAWrapper:\n    \"\"\"Gradient-norm aware global rank re-allocation wrapper for LoRA.\"\"\"\n\n    def __init__(self, hf_model: torch.nn.Module, r_init: int, total_rank: int, beta: float, warmup_steps: int):\n        self.model = get_peft_model(hf_model, LoraConfig(r=r_init))\n        self.total_rank = int(total_rank)\n        self.beta = beta\n        self.warmup_steps = warmup_steps\n        self.step_idx = 0\n\n        self.lora_modules = [m for m in self.model.modules() if hasattr(m, \"lora_A\") and hasattr(m, \"lora_B\")]\n        self.g_ema = torch.zeros(len(self.lora_modules))\n        for idx, mod in enumerate(self.lora_modules):\n            mod.lora_B.register_hook(self._make_grad_hook(idx))\n\n    # ---------------------------------------------------------------------\n    def _make_grad_hook(self, idx: int):\n        def _hook(grad):\n            gn = grad.norm().detach().cpu()\n            self.g_ema[idx] = self.beta * self.g_ema[idx] + (1 - self.beta) * gn\n            return grad\n        return _hook\n\n    # ---------------------------------------------------------------------\n    def step_callback(self):\n        self.step_idx += 1\n        if self.step_idx == self.warmup_steps:\n            self._reallocate()\n\n    # ---------------------------------------------------------------------\n    def _reallocate(self):\n        scores = self.g_ema / self.g_ema.sum()\n        new_ranks = torch.round(scores * self.total_rank).int().tolist()\n        diff = self.total_rank - sum(new_ranks)\n        for i in range(abs(diff)):\n            new_ranks[i % len(new_ranks)] += 1 if diff > 0 else -1\n\n        for new_r, module in zip(new_ranks, self.lora_modules):\n            old_r = module.lora_A.weight.size(0)\n            if new_r == old_r:\n                continue\n            keep = min(old_r, new_r)\n            with torch.no_grad():\n                A_old = module.lora_A.weight.data[:keep].clone()\n                B_old = module.lora_B.weight.data[:keep].clone()\n                module.update_layer(r=new_r)\n                module.lora_A.weight.data[:keep] = A_old\n                module.lora_B.weight.data[:keep] = B_old\n        print(\"[GLoRA] Re-allocated ranks →\", new_ranks)\n\n\n# -----------------------------------------------------------------------------\n# Public builder --------------------------------------------------------------\n# -----------------------------------------------------------------------------\n\ndef build_model(cfg: Any, num_labels: int):\n    base = AutoModelForSequenceClassification.from_pretrained(cfg.model.backbone, num_labels=num_labels, cache_dir=CACHE_DIR)\n\n    method = str(cfg.method_name).lower()\n    if method == \"glora\":\n        return GLoRAWrapper(\n            base,\n            r_init=cfg.model.peft.init_rank,\n            total_rank=cfg.glo_ra.total_rank_budget,\n            beta=float(cfg.glo_ra.beta_ema),\n            warmup_steps=int(cfg.glo_ra.warmup_steps),\n        )\n    elif method == \"adalora\":\n        ada_cfg = AdaLoraConfig(\n            r=cfg.model.peft.init_rank,\n            target_r=cfg.adalora.total_rank_budget,\n            lora_alpha=16,\n            beta1=cfg.adalora.beta1,\n            beta2=cfg.adalora.beta2,\n            orth_reg_weight=cfg.adalora.orth_reg_weight,\n            tinit=cfg.adalora.adaptation_start_step,\n        )\n        return get_peft_model(base, ada_cfg)\n    else:  # vanilla LoRA baseline\n        lora_cfg = LoraConfig(r=cfg.model.peft.init_rank, lora_alpha=16)\n        return get_peft_model(base, lora_cfg)",
    "main_py": "\"\"\"src/main.py\nMain orchestrator – applies mode-specific overrides and spawns ``src.train`` as\nsub-process so that each run is executed in its own Hydra context.\n\"\"\"\nfrom __future__ import annotations\n\nimport subprocess\nimport sys\n\nimport hydra\nfrom omegaconf import DictConfig\n\n\n@hydra.main(config_path=\"../config\", config_name=\"config\")\ndef main(cfg: DictConfig) -> None:\n    if \"run\" not in cfg or cfg.run is None:\n        print(\"[Main] You must provide run=<run_id> (file in config/run/)\")\n        sys.exit(1)\n\n    overrides = [\n        f\"run={cfg.run}\",\n        f\"results_dir={cfg.results_dir}\",\n        f\"mode={cfg.mode}\",\n    ]\n\n    if cfg.mode == \"trial\":\n        overrides += [\n            \"wandb.mode=disabled\",\n            \"training.epochs=1\",\n            \"optuna.n_trials=0\",\n            \"training.max_train_batches=2\",\n            \"training.max_val_batches=2\",\n        ]\n    elif cfg.mode == \"full\":\n        overrides += [\"wandb.mode=online\"]\n    else:\n        print(f\"[Main] Unknown mode '{cfg.mode}' – expected 'trial' or 'full'\")\n        sys.exit(1)\n\n    cmd = [\n        sys.executable,\n        \"-u\",\n        \"-m\",\n        \"src.train\",\n    ] + overrides\n\n    print(\"[Main] Exec:\", \" \".join(cmd))\n    subprocess.run(cmd, check=True)\n\n\nif __name__ == \"__main__\":\n    main()",
    "pyproject_toml": "[project]\nname = \"glora_experiment\"\nversion = \"0.1.0\"\ndependencies = [\n    \"torch>=2.0.0\",\n    \"transformers>=4.37.0\",\n    \"datasets>=2.14.0\",\n    \"peft>=0.5.0\",\n    \"hydra-core>=1.3.2\",\n    \"wandb>=0.16.0\",\n    \"optuna>=3.4.0\",\n    \"pandas>=2.0.0\",\n    \"matplotlib>=3.7.0\",\n    \"seaborn>=0.13.0\",\n    \"scipy>=1.11.0\",\n    \"scikit-learn>=1.3.0\",\n    \"tqdm>=4.66.0\",\n]",
    "config_yaml": "# config/config.yaml\n# Global Hydra configuration shared across all scripts\n\ndefaults:\n  - _self_\n  - run: null\n\n# ---------------------------------------------------------------------------\n# WandB credentials (overridden to 'disabled' for trial mode) ---------------\nwandb:\n  entity: gengaru617-personal\n  project: 251106-test\n  mode: online\n\n# ---------------------------------------------------------------------------\n# Generic training defaults --------------------------------------------------\ntraining:\n  epochs: 3\n  learning_rate: 2e-4\n  weight_decay: 0.01\n  lr_warmup_steps: 1000\n  gradient_accumulation_steps: 1\n  max_grad_norm: 1.0\n  max_train_batches: null   # limited in trial mode\n  max_val_batches: null     # limited in trial mode\n\n# ---------------------------------------------------------------------------\n# Optuna (disabled unless n_trials > 0) --------------------------------------\noptuna:\n  n_trials: 0\n  direction: maximize\n  search_space: {}\n\n# ---------------------------------------------------------------------------\n# Execution control ----------------------------------------------------------\nmode: full            # full / trial\nresults_dir: ./results\n\n# End of global config -------------------------------------------------------"
}
